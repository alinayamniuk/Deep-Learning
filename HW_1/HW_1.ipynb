{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "–Ø–º–Ω—é–∫ –ê–ª—ñ–Ω–∞, –ö–ù-410"
      ],
      "metadata": {
        "id": "Ld2LmrBPalSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Manipulation**"
      ],
      "metadata": {
        "id": "jKpJV7SqavC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to get anything done, we need some way to store and manipulate data. Generally, there are two important things we need to do with data: (i) acquire them; and (ii) process them once they are inside the computer. There is no point in acquiring data without some way to store it, so let us get our hands dirty first by playing with synthetic data. To start, we introduce the  ùëõ - dimensional array, which is also called the tensor.\n",
        "\n",
        "If you have worked with NumPy, the most widely-used scientific computing package in Python, then you will find this section familiar. No matter which framework you use, its tensor class (ndarray in MXNet, Tensor in both PyTorch and TensorFlow) is similar to NumPy‚Äôs ndarray with a few killer features. First, GPU is well-supported to accelerate the computation whereas NumPy only supports CPU computation. Second, the tensor class supports automatic differentiation. These properties make the tensor class suitable for deep learning. Throughout the book, when we say tensors, we are referring to instances of the tensor class unless otherwise stated."
      ],
      "metadata": {
        "id": "SW1-oqwqbD_O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Getting** **Started**"
      ],
      "metadata": {
        "id": "Wd4qxPaja5sZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we aim to get you up and running, equipping you with the basic math and numerical computing tools that you will build on as you progress through the book. Do not worry if you struggle to grok some of the mathematical concepts or library functions. The following sections will revisit this material in the context of practical examples and it will sink in. On the other hand, if you already have some background and want to go deeper into the mathematical content, just skip this section."
      ],
      "metadata": {
        "id": "z69ps65Fbavs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DI84-0gYacA2"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(12)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "414JN-JFbT12",
        "outputId": "ad0bc0a0-8026-42f3-fd09-a8492f06bf3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqwFl6x1bcmO",
        "outputId": "7f1a2d6d-0dd1-4732-a47c-5e4b306c4d8c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([12])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D5jwp91nbhHh",
        "outputId": "ea5b17f0-ba48-419a-ca63-e4938071e426"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "12"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = x.reshape(3, 4)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joxk6FiTbwqR",
        "outputId": "d461bc6c-c1b3-4a55-914d-9790b3ceb3e9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.zeros((2, 3, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HewEgg9ubwtJ",
        "outputId": "b22999c5-47a9-43e5-a23e-9ab4935863a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]],\n",
              "\n",
              "        [[0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.],\n",
              "         [0., 0., 0., 0.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones((2, 3, 4))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PrMlyFLKbwvw",
        "outputId": "f0949f54-50b7-40bc-f978-87176630a716"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]],\n",
              "\n",
              "        [[1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.],\n",
              "         [1., 1., 1., 1.]]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn(3, 4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEBHFCaFbwyG",
        "outputId": "1aa268ac-9fbb-420b-8bd1-9205b557aa49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.7798,  1.8056,  1.8577,  0.4676],\n",
              "        [ 0.5006,  0.6591, -0.5079, -0.8976],\n",
              "        [-0.2426, -0.1884, -0.4473, -1.3130]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.tensor([[2, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QuT-Uc4Lbw0-",
        "outputId": "81029c89-999d-49a0-cf67-b625e1c1279e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[2, 1, 4, 3],\n",
              "        [1, 2, 3, 4],\n",
              "        [4, 3, 2, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Operations**"
      ],
      "metadata": {
        "id": "AoqlMe5Eb-UT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This book is not about software engineering. Our interests are not limited to simply reading and writing data from/to arrays. We want to perform mathematical operations on those arrays. Some of the simplest and most useful operations are the elementwise operations. These apply a standard scalar operation to each element of an array. For functions that take two arrays as inputs, elementwise operations apply some standard binary operator on each pair of corresponding elements from the two arrays. We can create an elementwise function from any function that maps from a scalar to a scalar.\n",
        "\n",
        "In mathematical notation, we would denote such a unary scalar operator (taking one input) by the signature  ùëì:‚Ñù‚Üí‚Ñù . This just means that the function is mapping from any real number ( ‚Ñù ) onto another. Likewise, we denote a binary scalar operator (taking two real inputs, and yielding one output) by the signature  ùëì:‚Ñù,‚Ñù‚Üí‚Ñù . Given any two vectors  ùêÆ  and  ùêØ  of the same shape, and a binary operator  ùëì , we can produce a vector  ùêú=ùêπ(ùêÆ,ùêØ)  by setting  ùëêùëñ‚Üêùëì(ùë¢ùëñ,ùë£ùëñ)  for all  ùëñ , where  ùëêùëñ,ùë¢ùëñ , and  ùë£ùëñ  are the  ùëñth  elements of vectors  ùêú,ùêÆ , and  ùêØ . Here, we produced the vector-valued  ùêπ:‚Ñùùëë,‚Ñùùëë‚Üí‚Ñùùëë  by lifting the scalar function to an elementwise vector operation.\n",
        "\n",
        "The common standard arithmetic operators (+, -, *, /, and **) have all been lifted to elementwise operations for any identically-shaped tensors of arbitrary shape. We can call elementwise operations on any two tensors of the same shape. In the following example, we use commas to formulate a 5-element tuple, where each element is the result of an elementwise operation."
      ],
      "metadata": {
        "id": "6AFlDWyFcECp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([1.0, 2, 4, 8])\n",
        "y = torch.tensor([2, 2, 2, 2])\n",
        "x + y, x - y, x * y, x / y, x ** y  # The ** operator is exponentiation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5nHYzY0bw3T",
        "outputId": "46f9e777-2571-4766-f7c8-0b52e9f012ee"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 3.,  4.,  6., 10.]),\n",
              " tensor([-1.,  0.,  2.,  6.]),\n",
              " tensor([ 2.,  4.,  8., 16.]),\n",
              " tensor([0.5000, 1.0000, 2.0000, 4.0000]),\n",
              " tensor([ 1.,  4., 16., 64.]))"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.exp(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ow_OBTlbw5Z",
        "outputId": "e8521a4f-7af7-45af-d01c-d3cefa67db0e"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2.7183e+00, 7.3891e+00, 5.4598e+01, 2.9810e+03])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(12, dtype=torch.float32).reshape((3,4))\n",
        "Y = torch.tensor([[2.0, 1, 4, 3], [1, 2, 3, 4], [4, 3, 2, 1]])\n",
        "torch.cat((X, Y), dim=0), torch.cat((X, Y), dim=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-a6AOZwcRY0",
        "outputId": "22a8a940-528d-4eb5-dac2-3f36388e6636"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [ 2.,  1.,  4.,  3.],\n",
              "         [ 1.,  2.,  3.,  4.],\n",
              "         [ 4.,  3.,  2.,  1.]]),\n",
              " tensor([[ 0.,  1.,  2.,  3.,  2.,  1.,  4.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.,  1.,  2.,  3.,  4.],\n",
              "         [ 8.,  9., 10., 11.,  4.,  3.,  2.,  1.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X == Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONiIQe3gcRbs",
        "outputId": "e7724e6f-3836-4a06-d777-cb22b4535719"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[False,  True, False,  True],\n",
              "        [False, False, False, False],\n",
              "        [False, False, False, False]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFeRYdcecReS",
        "outputId": "68ffead2-6b54-457a-8ef7-da711473d160"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(66.)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Broadcasting Mechanism**"
      ],
      "metadata": {
        "id": "WT0xWmxEcagN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above section, we saw how to perform elementwise operations on two tensors of the same shape. Under certain conditions, even when shapes differ, we can still perform elementwise operations by invoking the broadcasting mechanism. This mechanism works in the following way: First, expand one or both arrays by copying elements appropriately so that after this transformation, the two tensors have the same shape. Second, carry out the elementwise operations on the resulting arrays."
      ],
      "metadata": {
        "id": "NaClTRjgceog"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.arange(3).reshape((3, 1))\n",
        "b = torch.arange(2).reshape((1, 2))\n",
        "a, b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOde0gJTcRgp",
        "outputId": "492f4d19-78c9-43e8-a74e-004fe125b05c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0],\n",
              "         [1],\n",
              "         [2]]), tensor([[0, 1]]))"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a + b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "meJKFvJtcRjQ",
        "outputId": "502c2e21-4cbd-42c0-dbb5-6e26377f8fe0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 1],\n",
              "        [1, 2],\n",
              "        [2, 3]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Indexing and Slicing**\n",
        "Just as in any other Python array, elements in a tensor can be accessed by index. As in any Python array, the first element has index 0 and ranges are specified to include the first but before the last element. As in standard Python lists, we can access elements according to their relative position to the end of the list by using negative indices."
      ],
      "metadata": {
        "id": "7Pr0iEb8cmqy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X[-1], X[1:3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9a0Xec0crc1",
        "outputId": "c91a65d8-7446-4afd-e16a-52a14d2c6ece"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([[ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[1, 2] = 9\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1dMvFzFJcrqW",
        "outputId": "f0830623-2edd-4c08-da76-6a302e7e3e29"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  5.,  9.,  7.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0:2, :] = 12\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvfECp7wcrxn",
        "outputId": "b9d19471-22bc-446f-9afd-66144dbad519"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[12., 12., 12., 12.],\n",
              "        [12., 12., 12., 12.],\n",
              "        [ 8.,  9., 10., 11.]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Saving Memory**\n",
        "\n",
        "Running operations can cause new memory to be allocated to host results. For example, if we write Y = X + Y, we will dereference the tensor that Y used to point to and instead point Y at the newly allocated memory. In the following example, we demonstrate this with Python‚Äôs id() function, which gives us the exact address of the referenced object in memory. After running Y = Y + X, we will find that id(Y) points to a different location. That is because Python first evaluates Y + X, allocating new memory for the result and then makes Y point to this new location in memory."
      ],
      "metadata": {
        "id": "ohPKqOHHc2Kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(Y)\n",
        "Y = Y + X\n",
        "id(Y) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrghem6oc6Or",
        "outputId": "dd4ffad4-bce1-464b-cf10-c07756c81841"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Z = torch.zeros_like(Y)\n",
        "print('id(Z):', id(Z))\n",
        "Z[:] = X + Y\n",
        "print('id(Z):', id(Z))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VadhYzInc6Um",
        "outputId": "a6358519-7f95-46ac-8d9a-fe4bbfd2814a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id(Z): 140292626138032\n",
            "id(Z): 140292626138032\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "before = id(X)\n",
        "X += Y\n",
        "id(X) == before"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7trHdT1Gc6Xe",
        "outputId": "cae8058d-4908-46d9-a54c-e094481967e4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversion to Other Python Objects**"
      ],
      "metadata": {
        "id": "wuFc2jgTdAiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = X.numpy()\n",
        "B = torch.from_numpy(A)\n",
        "type(A), type(B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s4pboqsdc6aF",
        "outputId": "f759a3f1-7b37-4b92-9dba-198ce5f6d47a"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, torch.Tensor)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([3.5])\n",
        "a, a.item(), float(a), int(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QV7J73RWdICw",
        "outputId": "f8edc4ba-038f-48a6-bf19-88ad6ae18bd0"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([3.5000]), 3.5, 3.5, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preprocessing**\n",
        "\n",
        "So far we have introduced a variety of techniques for manipulating data that are already stored in tensors. To apply deep learning to solving real-world problems, we often begin with preprocessing raw data, rather than those nicely prepared data in the tensor format. Among popular data analytic tools in Python, the pandas package is commonly used. Like many other extension packages in the vast ecosystem of Python, pandas can work together with tensors. So, we will briefly walk through steps for preprocessing raw data with pandas and converting them into the tensor format. We will cover more data preprocessing techniques in later chapters.\n",
        "\n"
      ],
      "metadata": {
        "id": "2F9YqD23dMAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reading the Dataset**"
      ],
      "metadata": {
        "id": "qtWWqyEJdZaL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.makedirs(os.path.join('..', 'data'), exist_ok=True)\n",
        "data_file = os.path.join('..', 'data', 'house_tiny.csv')\n",
        "with open(data_file, 'w') as f:\n",
        "    f.write('NumRooms,Alley,Price\\n')  # Column names\n",
        "    f.write('NA,Pave,127500\\n')  # Each row represents a data example\n",
        "    f.write('2,NA,106000\\n')\n",
        "    f.write('4,NA,178100\\n')\n",
        "    f.write('NA,NA,140000\\n')"
      ],
      "metadata": {
        "id": "vVfLdnSVdPSC"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If pandas is not installed, just uncomment the following line:\n",
        "# !pip install pandas\n",
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(data_file)\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "in2QNbTldPUm",
        "outputId": "d6ac69e1-79d9-4d95-8014-b9cb9d8cf436"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley   Price\n",
            "0       NaN  Pave  127500\n",
            "1       2.0   NaN  106000\n",
            "2       4.0   NaN  178100\n",
            "3       NaN   NaN  140000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Handling Missing Data**\n",
        "\n",
        "Note that ‚ÄúNaN‚Äù entries are missing values. To handle missing data, typical methods include imputation and deletion, where imputation replaces missing values with substituted ones, while deletion ignores missing values. Here we will consider imputation."
      ],
      "metadata": {
        "id": "qOm_VUSwdiBA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs, outputs = data.iloc[:, 0:2], data.iloc[:, 2]\n",
        "inputs = inputs.fillna(inputs.mean())\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IIh2a9aLdPXO",
        "outputId": "f6fcf85f-52b6-49d4-efcc-0b00a483ec8e"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms Alley\n",
            "0       3.0  Pave\n",
            "1       2.0   NaN\n",
            "2       4.0   NaN\n",
            "3       3.0   NaN\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = pd.get_dummies(inputs, dummy_na=True)\n",
        "print(inputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-g4HdVoudPZ0",
        "outputId": "1ae7f75c-725a-405c-a198-22ff22c7ca19"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   NumRooms  Alley_Pave  Alley_nan\n",
            "0       3.0           1          0\n",
            "1       2.0           0          1\n",
            "2       4.0           0          1\n",
            "3       3.0           0          1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Conversion to the Tensor Format**\n",
        "\n"
      ],
      "metadata": {
        "id": "a478uRzRdsFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = torch.tensor(inputs.values), torch.tensor(outputs.values)\n",
        "X, y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvFp4XKNdxQh",
        "outputId": "c15b0eef-3538-44de-bd90-42e81408a6d1"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[3., 1., 0.],\n",
              "         [2., 0., 1.],\n",
              "         [4., 0., 1.],\n",
              "         [3., 0., 1.]], dtype=torch.float64),\n",
              " tensor([127500, 106000, 178100, 140000]))"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Linear Algebra**\n",
        "\n",
        "Now that you can store and manipulate data, let us briefly review the subset of basic linear algebra that you will need to understand and implement most of models covered in this book. Below, we introduce the basic mathematical objects, arithmetic, and operations in linear algebra, expressing each of them through mathematical notation and the corresponding implementation in code."
      ],
      "metadata": {
        "id": "5c6XQ04yeFP8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scalars**\n",
        "\n",
        "If you never studied linear algebra or machine learning, then your past experience with math probably consisted of thinking about one number at a time. And, if you ever balanced a checkbook or even paid for dinner at a restaurant then you already know how to do basic things like adding and multiplying pairs of numbers. For example, the temperature in Palo Alto is  52  degrees Fahrenheit. Formally, we call values consisting of just one numerical quantity scalars. If you wanted to convert this value to Celsius (the metric system‚Äôs more sensible temperature scale), you would evaluate the expression  ùëê = 5/9(ùëì‚àí32) , setting  ùëì to 52 . In this equation, each of the terms ‚Äî 5, 9, and  32 ‚Äî are scalar values. The placeholders  ùëê  and  ùëì  are called variables and they represent unknown scalar values.\n",
        "\n",
        "In this book, we adopt the mathematical notation where scalar variables are denoted by ordinary lower-cased letters (e.g.,  ùë• ,  ùë¶ , and  ùëß ). We denote the space of all (continuous) real-valued scalars by  ‚Ñù . For expedience, we will punt on rigorous definitions of what precisely space is, but just remember for now that the expression  ùë•‚àà‚Ñù  is a formal way to say that  ùë•  is a real-valued scalar. The symbol  ‚àà  can be pronounced ‚Äúin‚Äù and simply denotes membership in a set. Analogously, we could write  ùë•, ùë¶ ‚àà {0,1}  to state that  ùë•  and  ùë¶  are numbers whose value can only be  0  or  1 .\n",
        "\n",
        "A scalar is represented by a tensor with just one element. In the next snippet, we instantiate two scalars and perform some familiar arithmetic operations with them, namely addition, multiplication, division, and exponentiation."
      ],
      "metadata": {
        "id": "-HyoS1hZeK-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor(3.0)\n",
        "y = torch.tensor(2.0)\n",
        "\n",
        "x + y, x * y, x / y, x**y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L7goRVu3efBZ",
        "outputId": "af96af9f-983a-435f-bd08-7278bc11eb8b"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(5.), tensor(6.), tensor(1.5000), tensor(9.))"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Vectors**\n",
        "\n",
        "You can think of a vector as simply a list of scalar values. We call these values the elements (entries or components) of the vector. When our vectors represent examples from our dataset, their values hold some real-world significance. For example, if we were training a model to predict the risk that a loan defaults, we might associate each applicant with a vector whose components correspond to their income, length of employment, number of previous defaults, and other factors. If we were studying the risk of heart attacks hospital patients potentially face, we might represent each patient by a vector whose components capture their most recent vital signs, cholesterol levels, minutes of exercise per day, etc. In math notation, we will usually denote vectors as bold-faced, lower-cased letters (e.g.,  ùê± ,  ùê≤ , and  ùê≥) ."
      ],
      "metadata": {
        "id": "NxCEKoaEhfAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Me-g3tidefD_",
        "outputId": "45ba332b-9098-4ceb-cd6f-b4c570f653c5"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTFvoFnhefG_",
        "outputId": "86938e3b-724b-4ad0-9a3c-609969b25b5a"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3)"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfDg1pLxh2PG",
        "outputId": "c0a87251-59bb-45ba-806e-a230b420ceb5"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yiFzaW5Vh5Ar",
        "outputId": "a2b768ec-9093-43d8-fb4f-ffe48f837d74"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Matrices**\n",
        "\n",
        "Just as vectors generalize scalars from order zero to order one, matrices generalize vectors from order one to order two. Matrices, which we will typically denote with bold-faced, capital letters (e.g.,  ùêó ,  ùêò , and  ùêô ), are represented in code as tensors with two axes.\n",
        "\n",
        "In math notation, we use  ùêÄ‚àà‚Ñùùëö√óùëõ  to express that the matrix  ùêÄ  consists of  ùëö  rows and  ùëõ  columns of real-valued scalars. Visually, we can illustrate any matrix  ùêÄ‚àà‚Ñùùëö√óùëõ  as a table, where each element  ùëéùëñùëó  belongs to the  ùëñth  row and  ùëóth  column"
      ],
      "metadata": {
        "id": "MnrYdJl4huew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20).reshape(5, 4)\n",
        "A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZSYUIAoiUW6",
        "outputId": "c5e1ea43-16e3-4147-c4e6-c9e38a813ec2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  1,  2,  3],\n",
              "        [ 4,  5,  6,  7],\n",
              "        [ 8,  9, 10, 11],\n",
              "        [12, 13, 14, 15],\n",
              "        [16, 17, 18, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ys7U01o_iiPQ",
        "outputId": "4ee0b1bf-e731-475d-96a5-41d5d56f735b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0,  4,  8, 12, 16],\n",
              "        [ 1,  5,  9, 13, 17],\n",
              "        [ 2,  6, 10, 14, 18],\n",
              "        [ 3,  7, 11, 15, 19]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.tensor([[1, 2, 3], [2, 0, 4], [3, 4, 5]])\n",
        "B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5cvgvd3ikkK",
        "outputId": "1b377b89-6152-4c7b-e7f4-51d97c0294cc"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [2, 0, 4],\n",
              "        [3, 4, 5]])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B == B.T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6gi7fkPiosh",
        "outputId": "40703319-34f2-4be0-b57a-d5e5209edad7"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[True, True, True],\n",
              "        [True, True, True],\n",
              "        [True, True, True]])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tensors**\n",
        "\n",
        "Just as vectors generalize scalars, and matrices generalize vectors, we can build data structures with even more axes. Tensors (‚Äútensors‚Äù in this subsection refer to algebraic objects) give us a generic way of describing  ùëõ -dimensional arrays with an arbitrary number of axes. Vectors, for example, are first-order tensors, and matrices are second-order tensors."
      ],
      "metadata": {
        "id": "tB06dfuiyVVS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Amzo8jzayfa1",
        "outputId": "a18549c7-33d6-412b-d950-cc91b69273f8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0,  1,  2,  3],\n",
              "         [ 4,  5,  6,  7],\n",
              "         [ 8,  9, 10, 11]],\n",
              "\n",
              "        [[12, 13, 14, 15],\n",
              "         [16, 17, 18, 19],\n",
              "         [20, 21, 22, 23]]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
        "A, A + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3uTc7E8cyp93",
        "outputId": "ed6998ec-5311-48e4-872c-2d768433296a"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],\n",
              "         [ 8., 10., 12., 14.],\n",
              "         [16., 18., 20., 22.],\n",
              "         [24., 26., 28., 30.],\n",
              "         [32., 34., 36., 38.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rYq0QLP_y2M5",
        "outputId": "d29ce5fd-5c23-4f12-c1f1-9dc729246d8f"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic Properties of Tensor Arithmetic**\n",
        "\n",
        "Scalars, vectors, matrices, and tensors (‚Äútensors‚Äù in this subsection refer to algebraic objects) of an arbitrary number of axes have some nice properties that often come in handy. For example, you might have noticed from the definition of an elementwise operation that any elementwise unary operation does not change the shape of its operand. Similarly, given any two tensors with the same shape, the result of any binary elementwise operation will be a tensor of that same shape. For example, adding two matrices of the same shape performs elementwise addition over these two matrices."
      ],
      "metadata": {
        "id": "70bcveS0zDVy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
        "A, A + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vJEp4_QxzK1y",
        "outputId": "2a8ec869-06a2-45f7-9071-3f4a9e248f4f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],\n",
              "         [ 8., 10., 12., 14.],\n",
              "         [16., 18., 20., 22.],\n",
              "         [24., 26., 28., 30.],\n",
              "         [32., 34., 36., 38.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZrD9qlgRzQe7",
        "outputId": "b454fbac-1b38-483c-ee73-a8ca73a526a3"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = torch.arange(20, dtype=torch.float32).reshape(5, 4)\n",
        "B = A.clone()  # Assign a copy of `A` to `B` by allocating new memory\n",
        "A, A + B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MO7VyrnRzvXO",
        "outputId": "f149e409-7d6c-45a9-c976-8f07f2a4d421"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[ 0.,  1.,  2.,  3.],\n",
              "         [ 4.,  5.,  6.,  7.],\n",
              "         [ 8.,  9., 10., 11.],\n",
              "         [12., 13., 14., 15.],\n",
              "         [16., 17., 18., 19.]]), tensor([[ 0.,  2.,  4.,  6.],\n",
              "         [ 8., 10., 12., 14.],\n",
              "         [16., 18., 20., 22.],\n",
              "         [24., 26., 28., 30.],\n",
              "         [32., 34., 36., 38.]]))"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A * B"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUo4an6Kz-VP",
        "outputId": "1e2d6859-c69e-4a59-b82f-7e8beb735af2"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[  0.,   1.,   4.,   9.],\n",
              "        [ 16.,  25.,  36.,  49.],\n",
              "        [ 64.,  81., 100., 121.],\n",
              "        [144., 169., 196., 225.],\n",
              "        [256., 289., 324., 361.]])"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = 2\n",
        "X = torch.arange(24).reshape(2, 3, 4)\n",
        "a + X, (a * X).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3C581usz-YE",
        "outputId": "b0efce9f-139f-4090-840b-f501bdb62f1a"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[[ 2,  3,  4,  5],\n",
              "          [ 6,  7,  8,  9],\n",
              "          [10, 11, 12, 13]],\n",
              " \n",
              "         [[14, 15, 16, 17],\n",
              "          [18, 19, 20, 21],\n",
              "          [22, 23, 24, 25]]]), torch.Size([2, 3, 4]))"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reduction**\n",
        "\n",
        "One useful operation that we can perform with arbitrary tensors is to calculate the sum of their elements. In mathematical notation, we express sums using the  ‚àë  symbol. To express the sum of the elements in a vector  ùê±  of length  ùëë , we write  ‚àëùëëùëñ=1ùë•ùëñ . In code, we can just call the function for calculating the sum."
      ],
      "metadata": {
        "id": "w1KE37UM0ESx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4, dtype=torch.float32)\n",
        "x, x.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP18uLv4z-cA",
        "outputId": "ef4a18a1-acc0-479a-aefc-cf5ba0c852b6"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, A.sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XyTE-bD-0M6D",
        "outputId": "4e447b5f-9d6d-4deb-e00a-9794c9d15396"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), tensor(190.))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_sum_axis0 = A.sum(axis=0)\n",
        "A_sum_axis0, A_sum_axis0.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-1pnV50M9Z",
        "outputId": "658d0c31-8b20-4b9d-e961-51be67c0eeca"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([40., 45., 50., 55.]), torch.Size([4]))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A_sum_axis1 = A.sum(axis=1)\n",
        "A_sum_axis1, A_sum_axis1.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8DdoQOT0RnE",
        "outputId": "2c15238e-3b33-4777-8c2b-0ed72bfd632e"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 6., 22., 38., 54., 70.]), torch.Size([5]))"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.sum(axis=[0, 1])  # Same as `A.sum()`"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6KRguwZ0UEG",
        "outputId": "8075ffda-ca54-4427-cc73-7253d7d61d48"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(190.)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(), A.sum() / A.numel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOLk8tpF0WlU",
        "outputId": "40815793-da1d-4d9a-fba6-f45934941ec9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(9.5000), tensor(9.5000))"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.mean(axis=0), A.sum(axis=0) / A.shape[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lp0mspm-0ZR9",
        "outputId": "bab9ea0a-3d4e-405d-cf41-020ab6cb6770"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 8.,  9., 10., 11.]), tensor([ 8.,  9., 10., 11.]))"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Non-Reduction Sum**\n",
        "\n",
        "However, sometimes it can be useful to keep the number of axes unchanged when invoking the function for calculating the sum or mean."
      ],
      "metadata": {
        "id": "UBjyhPK30b8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum_A = A.sum(axis=1, keepdims=True)\n",
        "sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGms3VAw0e2-",
        "outputId": "e4550bb3-f9bc-4ffb-f572-1c584345d50a"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.],\n",
              "        [22.],\n",
              "        [38.],\n",
              "        [54.],\n",
              "        [70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A / sum_A"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKaNmgHw0n-k",
        "outputId": "65a9babf-5415-41e0-b71d-5015311fd979"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0000, 0.1667, 0.3333, 0.5000],\n",
              "        [0.1818, 0.2273, 0.2727, 0.3182],\n",
              "        [0.2105, 0.2368, 0.2632, 0.2895],\n",
              "        [0.2222, 0.2407, 0.2593, 0.2778],\n",
              "        [0.2286, 0.2429, 0.2571, 0.2714]])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A.cumsum(axis=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f-muXZsj0qoa",
        "outputId": "eadead14-acbc-438a-9792-41550536101c"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.,  1.,  2.,  3.],\n",
              "        [ 4.,  6.,  8., 10.],\n",
              "        [12., 15., 18., 21.],\n",
              "        [24., 28., 32., 36.],\n",
              "        [40., 45., 50., 55.]])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dot Products**\n",
        "\n",
        "So far, we have only performed elementwise operations, sums, and averages. And if this was all we could do, linear algebra probably would not deserve its own section. However, one of the most fundamental operations is the dot product. Given two vectors  ùê±,ùê≤‚àà‚Ñùùëë , their dot product  ùê±‚ä§ùê≤  (or  ‚ü®ùê±,ùê≤‚ü© ) is a sum over the products of the elements at the same position:  ùê±‚ä§ùê≤=‚àëùëëùëñ=1ùë•ùëñùë¶ùëñ ."
      ],
      "metadata": {
        "id": "ttJJm5vx00b3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = torch.ones(4, dtype = torch.float32)\n",
        "x, y, torch.dot(x, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDdbbt1n0s5v",
        "outputId": "1ddc7cef-388b-4031-f77f-123ffc3dcde5"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([0., 1., 2., 3.]), tensor([1., 1., 1., 1.]), tensor(6.))"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.sum(x * y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYNVRcTW06zZ",
        "outputId": "ebb431a4-13d9-4170-ab9b-2cf86fcd7a62"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Matrix-Vector Products**"
      ],
      "metadata": {
        "id": "oonljz1H1AVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "A.shape, x.shape, torch.mv(A, x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dSBa-XYr1B7W",
        "outputId": "a0ea0ceb-2da2-4f7b-ad4a-edb47e4b5da5"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([5, 4]), torch.Size([4]), tensor([ 14.,  38.,  62.,  86., 110.]))"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Matrix-Matrix Multiplication**"
      ],
      "metadata": {
        "id": "4sxRQVRI1Hkn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = torch.ones(4, 3)\n",
        "torch.mm(A, B)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "du_U7SFm1B-e",
        "outputId": "f8f3d9cc-908e-487e-914c-8974b9825f01"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 6.,  6.,  6.],\n",
              "        [22., 22., 22.],\n",
              "        [38., 38., 38.],\n",
              "        [54., 54., 54.],\n",
              "        [70., 70., 70.]])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CcH75HoS1NP9"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Norms**\n",
        "\n",
        "Some of the most useful operators in linear algebra are norms. Informally, the norm of a vector tells us how big a vector is. The notion of size under consideration here concerns not dimensionality but rather the magnitude of the components.\n",
        "\n",
        "In linear algebra, a vector norm is a function  ùëì  that maps a vector to a scalar, satisfying a handful of properties. Given any vector  ùê± , the first property says that if we scale all the elements of a vector by a constant factor  ùõº , its norm also scales by the absolute value of the same constant factor:\n",
        "\n",
        "ùëì(ùõºùê±)=|ùõº|ùëì(ùê±).\n",
        " \n",
        "The second property is the familiar triangle inequality:\n",
        "\n",
        "ùëì(ùê±+ùê≤)‚â§ùëì(ùê±)+ùëì(ùê≤).\n",
        " \n",
        "The third property simply says that the norm must be non-negative:\n",
        "\n",
        "ùëì(ùê±)‚â•0.\n",
        " \n",
        "That makes sense, as in most contexts the smallest size for anything is 0. The final property requires that the smallest norm is achieved and only achieved by a vector consisting of all zeros.\n",
        "\n",
        "‚àÄùëñ,[ùê±]ùëñ=0‚áîùëì(ùê±)=0."
      ],
      "metadata": {
        "id": "Ptdq_qbc1M1T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "u = torch.tensor([3.0, -4.0])\n",
        "torch.norm(u)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIQi10941CBG",
        "outputId": "42926840-f60a-4da9-9a7d-39933d64c4fe"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(5.)"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.abs(u).sum()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nS3qn1B1CDu",
        "outputId": "8e1b576b-7a53-40b4-bcdc-218d1c96a837"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(7.)"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.norm(torch.ones((4, 9)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pj3VqSQA1fMP",
        "outputId": "a782461e-4653-4200-dd61-7404401bc362"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(6.)"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Calculus**"
      ],
      "metadata": {
        "id": "_MQuW85K2E2S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  Finding the area of a polygon had remained mysterious until at least 2,500 years ago, when ancient Greeks divided a polygon into triangles and summed their areas. To find the area of curved shapes, such as a circle, ancient Greeks inscribed polygons in such shapes.\n",
        "  In deep learning, we train models, updating them successively so that they get better and better as they see more and more data. Usually, getting better means minimizing a loss function, a score that answers the question ‚Äúhow bad is our model?‚Äù This question is more subtle than it appears. Ultimately, what we really care about is producing a model that performs well on data that we have never seen before. But we can only fit the model to data that we can actually see. Thus we can decompose the task of fitting models into two key concerns: (i) optimization: the process of fitting our models to observed data; (ii) generalization: the mathematical principles and practitioners‚Äô wisdom that guide as to how to produce models whose validity extends beyond the exact set of data examples used to train them."
      ],
      "metadata": {
        "id": "Fdhu-pta2OBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Derivatives and Differentiation**"
      ],
      "metadata": {
        "id": "5IJ2wg_t2hfE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We begin by addressing the calculation of derivatives, a crucial step in nearly all deep learning optimization algorithms. In deep learning, we typically choose loss functions that are differentiable with respect to our model‚Äôs parameters. Put simply, this means that for each parameter, we can determine how rapidly the loss would increase or decrease, were we to increase or decrease that parameter by an infinitesimally small amount."
      ],
      "metadata": {
        "id": "0aNHPIOZ2nMy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -U d2l\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "from IPython import display\n",
        "from d2l import torch as d2l\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    return 3 * x ** 2 - 4 * x"
      ],
      "metadata": {
        "id": "xlUN0mBb2qt7"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_lim(f, x, h):\n",
        "    return (f(x + h) - f(x)) / h\n",
        "\n",
        "h = 0.1\n",
        "for i in range(5):\n",
        "    print(f'h={h:.5f}, numerical limit={numerical_lim(f, 1, h):.5f}')\n",
        "    h *= 0.1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCYNZm503TrE",
        "outputId": "99ef3675-ac69-482f-f366-eee81d35bf97"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "h=0.10000, numerical limit=2.30000\n",
            "h=0.01000, numerical limit=2.03000\n",
            "h=0.00100, numerical limit=2.00300\n",
            "h=0.00010, numerical limit=2.00030\n",
            "h=0.00001, numerical limit=2.00003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def use_svg_display():\n",
        "    \"\"\"Use the svg format to display a plot in Jupyter.\"\"\"\n",
        "    display.set_matplotlib_formats('svg')"
      ],
      "metadata": {
        "id": "zz5lUWOg3WOl"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_figsize(figsize=(3.5, 2.5)):\n",
        "    \"\"\"Set the figure size for matplotlib.\"\"\"\n",
        "    use_svg_display()\n",
        "    d2l.plt.rcParams['figure.figsize'] = figsize"
      ],
      "metadata": {
        "id": "md7svS_Y3Zc_"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend):\n",
        "    \"\"\"Set the axes for matplotlib.\"\"\"\n",
        "    axes.set_xlabel(xlabel)\n",
        "    axes.set_ylabel(ylabel)\n",
        "    axes.set_xscale(xscale)\n",
        "    axes.set_yscale(yscale)\n",
        "    axes.set_xlim(xlim)\n",
        "    axes.set_ylim(ylim)\n",
        "    if legend:\n",
        "        axes.legend(legend)\n",
        "    axes.grid()"
      ],
      "metadata": {
        "id": "_-Yj9Lm73cKh"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot(X, Y=None, xlabel=None, ylabel=None, legend=None, xlim=None,\n",
        "         ylim=None, xscale='linear', yscale='linear',\n",
        "         fmts=('-', 'm--', 'g-.', 'r:'), figsize=(3.5, 2.5), axes=None):\n",
        "    \"\"\"Plot data points.\"\"\"\n",
        "    if legend is None:\n",
        "        legend = []\n",
        "\n",
        "    set_figsize(figsize)\n",
        "    axes = axes if axes else d2l.plt.gca()\n",
        "\n",
        "    # Return True if `X` (tensor or list) has 1 axis\n",
        "    def has_one_axis(X):\n",
        "        return (hasattr(X, \"ndim\") and X.ndim == 1 or isinstance(X, list)\n",
        "                and not hasattr(X[0], \"__len__\"))\n",
        "\n",
        "    if has_one_axis(X):\n",
        "        X = [X]\n",
        "    if Y is None:\n",
        "        X, Y = [[]] * len(X), X\n",
        "    elif has_one_axis(Y):\n",
        "        Y = [Y]\n",
        "    if len(X) != len(Y):\n",
        "        X = X * len(Y)\n",
        "    axes.cla()\n",
        "    for x, y, fmt in zip(X, Y, fmts):\n",
        "        if len(x):\n",
        "            axes.plot(x, y, fmt)\n",
        "        else:\n",
        "            axes.plot(y, fmt)\n",
        "    set_axes(axes, xlabel, ylabel, xlim, ylim, xscale, yscale, legend)"
      ],
      "metadata": {
        "id": "NFWEOOx53cCY"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.arange(0, 3, 0.1)\n",
        "plot(x, [f(x), 2 * x - 3], 'x', 'f(x)', legend=['f(x)', 'Tangent line (x=1)'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "ar3nMmuM3iUZ",
        "outputId": "55717e32-8e68-4d1f-8ae1-c5be62a50487"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 252x180 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"180.65625pt\" version=\"1.1\" viewBox=\"0 0 243.529359 180.65625\" width=\"243.529359pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-21T22:07:55.235417</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 180.65625 \nL 243.529359 180.65625 \nL 243.529359 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \nL 235.903125 7.2 \nL 40.603125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 49.480398 143.1 \nL 49.480398 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_2\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m110a0b0973\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"49.480398\" xlink:href=\"#m110a0b0973\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(46.299148 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_3\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 110.702968 143.1 \nL 110.702968 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"110.702968\" xlink:href=\"#m110a0b0973\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 1 -->\n      <g transform=\"translate(107.521718 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_5\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 171.925539 143.1 \nL 171.925539 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"171.925539\" xlink:href=\"#m110a0b0973\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 2 -->\n      <g transform=\"translate(168.744289 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_7\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 233.148109 143.1 \nL 233.148109 7.2 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"233.148109\" xlink:href=\"#m110a0b0973\" y=\"143.1\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 3 -->\n      <g transform=\"translate(229.966859 157.698438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- x -->\n     <g transform=\"translate(135.29375 171.376563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-120\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 40.603125 114.635514 \nL 235.903125 114.635514 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_10\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf8f297a163\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mf8f297a163\" y=\"114.635514\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 0 -->\n      <g transform=\"translate(27.240625 118.434732)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_11\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 40.603125 77.490157 \nL 235.903125 77.490157 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mf8f297a163\" y=\"77.490157\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 5 -->\n      <g transform=\"translate(27.240625 81.289376)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_13\">\n      <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 40.603125 40.344801 \nL 235.903125 40.344801 \n\" style=\"fill:none;stroke:#b0b0b0;stroke-linecap:square;stroke-width:0.8;\"/>\n     </g>\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"40.603125\" xlink:href=\"#mf8f297a163\" y=\"40.344801\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 10 -->\n      <g transform=\"translate(20.878125 44.14402)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- f(x) -->\n     <g transform=\"translate(14.798437 83.771094)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"74.21875\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"133.398438\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 49.480398 114.635514 \nL 55.602655 117.38427 \nL 61.724912 119.687282 \nL 67.847169 121.54455 \nL 73.969426 122.956073 \nL 80.091683 123.921853 \nL 86.21394 124.441888 \nL 92.336197 124.516178 \nL 98.458454 124.144725 \nL 104.580711 123.327527 \nL 110.702968 122.064585 \nL 116.825225 120.355898 \nL 122.947482 118.201468 \nL 129.069739 115.601293 \nL 135.191996 112.555374 \nL 141.314254 109.06371 \nL 147.436511 105.126302 \nL 153.558768 100.74315 \nL 159.681025 95.914254 \nL 165.803282 90.639614 \nL 171.925539 84.919229 \nL 178.047796 78.7531 \nL 184.170053 72.141226 \nL 190.29231 65.083608 \nL 196.414567 57.580247 \nL 202.536824 49.63114 \nL 208.659081 41.23629 \nL 214.781338 32.395695 \nL 220.903595 23.109356 \nL 227.025852 13.377273 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#pb3ea1f0c6e)\" d=\"M 49.480398 136.922727 \nL 55.602655 135.436913 \nL 61.724912 133.951099 \nL 67.847169 132.465285 \nL 73.969426 130.97947 \nL 80.091683 129.493656 \nL 86.21394 128.007842 \nL 92.336197 126.522028 \nL 98.458454 125.036213 \nL 104.580711 123.550399 \nL 110.702968 122.064585 \nL 116.825225 120.578771 \nL 122.947482 119.092956 \nL 129.069739 117.607142 \nL 135.191996 116.121328 \nL 141.314254 114.635514 \nL 147.436511 113.149699 \nL 153.558768 111.663885 \nL 159.681025 110.178071 \nL 165.803282 108.692257 \nL 171.925539 107.206442 \nL 178.047796 105.720628 \nL 184.170053 104.234814 \nL 190.29231 102.749 \nL 196.414567 101.263185 \nL 202.536824 99.777371 \nL 208.659081 98.291557 \nL 214.781338 96.805743 \nL 220.903595 95.319928 \nL 227.025852 93.834114 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 40.603125 143.1 \nL 40.603125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 235.903125 143.1 \nL 235.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 40.603125 143.1 \nL 235.903125 143.1 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 40.603125 7.2 \nL 235.903125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 47.603125 44.55625 \nL 172.153125 44.55625 \nQ 174.153125 44.55625 174.153125 42.55625 \nL 174.153125 14.2 \nQ 174.153125 12.2 172.153125 12.2 \nL 47.603125 12.2 \nQ 45.603125 12.2 45.603125 14.2 \nL 45.603125 42.55625 \nQ 45.603125 44.55625 47.603125 44.55625 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_17\">\n     <path d=\"M 49.603125 20.298437 \nL 69.603125 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_18\"/>\n    <g id=\"text_10\">\n     <!-- f(x) -->\n     <g transform=\"translate(77.603125 23.798437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"35.205078\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"74.21875\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"133.398438\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_19\">\n     <path d=\"M 49.603125 34.976562 \nL 69.603125 34.976562 \n\" style=\"fill:none;stroke:#bf00bf;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_20\"/>\n    <g id=\"text_11\">\n     <!-- Tangent line (x=1) -->\n     <g transform=\"translate(77.603125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M -0.296875 72.90625 \nL 61.375 72.90625 \nL 61.375 64.59375 \nL 35.5 64.59375 \nL 35.5 0 \nL 25.59375 0 \nL 25.59375 64.59375 \nL -0.296875 64.59375 \nz\n\" id=\"DejaVuSans-84\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 45.40625 27.984375 \nQ 45.40625 37.75 41.375 43.109375 \nQ 37.359375 48.484375 30.078125 48.484375 \nQ 22.859375 48.484375 18.828125 43.109375 \nQ 14.796875 37.75 14.796875 27.984375 \nQ 14.796875 18.265625 18.828125 12.890625 \nQ 22.859375 7.515625 30.078125 7.515625 \nQ 37.359375 7.515625 41.375 12.890625 \nQ 45.40625 18.265625 45.40625 27.984375 \nz\nM 54.390625 6.78125 \nQ 54.390625 -7.171875 48.1875 -13.984375 \nQ 42 -20.796875 29.203125 -20.796875 \nQ 24.46875 -20.796875 20.265625 -20.09375 \nQ 16.0625 -19.390625 12.109375 -17.921875 \nL 12.109375 -9.1875 \nQ 16.0625 -11.328125 19.921875 -12.34375 \nQ 23.78125 -13.375 27.78125 -13.375 \nQ 36.625 -13.375 41.015625 -8.765625 \nQ 45.40625 -4.15625 45.40625 5.171875 \nL 45.40625 9.625 \nQ 42.625 4.78125 38.28125 2.390625 \nQ 33.9375 0 27.875 0 \nQ 17.828125 0 11.671875 7.65625 \nQ 5.515625 15.328125 5.515625 27.984375 \nQ 5.515625 40.671875 11.671875 48.328125 \nQ 17.828125 56 27.875 56 \nQ 33.9375 56 38.28125 53.609375 \nQ 42.625 51.21875 45.40625 46.390625 \nL 45.40625 54.6875 \nL 54.390625 54.6875 \nz\n\" id=\"DejaVuSans-103\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-84\"/>\n      <use x=\"44.583984\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"105.863281\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"169.242188\" xlink:href=\"#DejaVuSans-103\"/>\n      <use x=\"232.71875\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"294.242188\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"357.621094\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"396.830078\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"428.617188\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"456.400391\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"484.183594\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"547.5625\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"609.085938\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"640.873047\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"679.886719\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"739.066406\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"822.855469\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"886.478516\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb3ea1f0c6e\">\n   <rect height=\"135.9\" width=\"195.3\" x=\"40.603125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Partial Derivatives**"
      ],
      "metadata": {
        "id": "lmo4uxbi3lHs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have dealt with the differentiation of functions of just one variable. In deep learning, functions often depend on many variables. Thus, we need to extend the ideas of differentiation to these multivariate functions.\n",
        "\n",
        "Let y=f(x1,x2,‚Ä¶,xn)\n",
        "be a function with n variables. The partial derivative of y with respect to its ith parameter xi\n",
        "\n",
        "‚àÇy‚àÇxi=limh‚Üí0f(x1,‚Ä¶,xi‚àí1,xi+h,xi+1,‚Ä¶,xn)‚àíf(x1,‚Ä¶,xi,‚Ä¶,xn)h.\n",
        "\n",
        "To calculate ‚àÇy‚àÇxi\n",
        ", we can simply treat x1,‚Ä¶,xi‚àí1,xi+1,‚Ä¶,xn as constants and calculate the derivative of y with respect to xi.\n",
        "\n",
        "For notation of partial derivatives, the following are equivalent:\n",
        "¬∂\n",
        "‚àÇy‚àÇxi=‚àÇf‚àÇxi=fxi=fi=Dif=Dxif."
      ],
      "metadata": {
        "id": "ZWV6gv1E3m1z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradients**"
      ],
      "metadata": {
        "id": "4YHDISPt3-Sq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can concatenate partial derivatives of a multivariate function with respect to all its variables to obtain the gradient vector of the function. Suppose that the input of function f:Rn‚ÜíR is an n-dimensional vector x=[x1,x2,‚Ä¶,xn]‚ä§ and the output is a scalar. The gradient of the function f(x) with respect to x is a vector of n partial derivatives"
      ],
      "metadata": {
        "id": "lwKFZNvq3_ws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chain Rule**"
      ],
      "metadata": {
        "id": "jlwVc4Cv4B4N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, such gradients can be hard to find. This is because multivariate functions in deep learning are often composite, so we may not apply any of the aforementioned rules to differentiate these functions. Fortunately, the chain rule enables us to differentiate composite functions."
      ],
      "metadata": {
        "id": "E0XzwH0b4ERp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**"
      ],
      "metadata": {
        "id": "pLrj2O3_4Gjs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Differential calculus and integral calculus are two branches of calculus, where the former can be applied to the ubiquitous optimization problems in deep learning.\n",
        "\n",
        "* A derivative can be interpreted as the instantaneous rate of change of a function with respect to its variable. It is also the slope of the tangent line to the curve of the function.\n",
        "\n",
        "* A gradient is a vector whose components are the partial derivatives of a multivariate function with respect to all its variables.\n",
        "\n",
        "* The chain rule enables us to differentiate composite functions\n"
      ],
      "metadata": {
        "id": "vkNZg45V4Ii9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Automatic Differentiation**"
      ],
      "metadata": {
        "id": "Hzdz_iby4Pc0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we have explained, differentiation is a crucial step in nearly all deep learning optimization algorithms. While the calculations for taking these derivatives are straightforward, requiring only some basic calculus, for complex models, working out the updates by hand can be a pain (and often error-prone).\n",
        "\n",
        "Deep learning frameworks expedite this work by automatically calculating derivatives, i.e., automatic differentiation. In practice, based on our designed model the system builds a computational graph, tracking which data combined through which operations to produce the output. Automatic differentiation enables the system to subsequently backpropagate gradients. Here, backpropagate simply means to trace through the computational graph, filling in the partial derivatives with respect to each parameter."
      ],
      "metadata": {
        "id": "Y7RiQdGV4UVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **A Simple Example**"
      ],
      "metadata": {
        "id": "8DmqD2zh4Wl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.arange(4.0)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdsmY0fl4aCQ",
        "outputId": "3ac71bc7-6731-43e6-869b-e7bfb921d866"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 1., 2., 3.])"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.requires_grad_(True)  # Same as `x = torch.arange(4.0, requires_grad=True)`\n",
        "x.grad  # The default value is None"
      ],
      "metadata": {
        "id": "zui_Ldff4d6O"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = 2 * torch.dot(x, x)\n",
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmqhnmZw4ggR",
        "outputId": "6e5e8612-85d5-4156-c9cf-0248b17fe263"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(28., grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMWb6ebe4iQR",
        "outputId": "0b8abc69-514e-4f6a-e4c1-fc752c23b14a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.,  4.,  8., 12.])"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad == 4 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el91iFE74jlp",
        "outputId": "b59226d9-1d8a-402e-c92d-eb818fe9941a"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch accumulates the gradient in default, we need to clear the previous\n",
        "# values\n",
        "x.grad.zero_()\n",
        "y = x.sum()\n",
        "y.backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ng5dmo_c4lD2",
        "outputId": "0879436c-6482-4708-818b-14dfa8097e93"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Backward for Non-Scalar Variables**"
      ],
      "metadata": {
        "id": "KgELRQgt4nir"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Technically, when y is not a scalar, the most natural interpretation of the differentiation of a vector y with respect to a vector x is a matrix. For higher-order and higher-dimensional y and x, the differentiation result could be a high-order tensor.\n",
        "\n",
        "However, while these more exotic objects do show up in advanced machine learning (including in deep learning), more often when we are calling backward on a vector, we are trying to calculate the derivatives of the loss functions for each constituent of a batch of training examples. Here, our intent is not to calculate the differentiation matrix but rather the sum of the partial derivatives computed individually for each example in the batch."
      ],
      "metadata": {
        "id": "IaN3s8nb4oy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Invoking `backward` on a non-scalar requires passing in a `gradient` argument\n",
        "# which specifies the gradient of the differentiated function w.r.t `self`.\n",
        "# In our case, we simply want to sum the partial derivatives, so passing\n",
        "# in a gradient of ones is appropriate\n",
        "x.grad.zero_()\n",
        "y = x * x\n",
        "# y.backward(torch.ones(len(x))) equivalent to the below\n",
        "y.sum().backward()\n",
        "x.grad"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJ17J-7D4qB-",
        "outputId": "a8cdaa41-5911-4dcb-b468-e03f9819872a"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 2., 4., 6.])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detaching Computation**"
      ],
      "metadata": {
        "id": "zFtgeyvN4tCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sometimes, we wish to move some calculations outside of the recorded computational graph. For example, say that y was calculated as a function of x, and that subsequently z was calculated as a function of both y and x. Now, imagine that we wanted to calculate the gradient of z with respect to x, but wanted for some reason to treat y as a constant, and only take into account the role that x played after y was calculated.\n",
        "\n",
        "Here, we can detach y to return a new variable u that has the same value as y but discards any information about how y was computed in the computational graph. In other words, the gradient will not flow backwards through u to x. Thus, the following backpropagation function computes the partial derivative of z = u * x with respect to x while treating u as a constant, instead of the partial derivative of z = x * x * x with respect to x."
      ],
      "metadata": {
        "id": "Nb5_yL3d4whw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y = x * x\n",
        "u = y.detach()\n",
        "z = u * x\n",
        "\n",
        "z.sum().backward()\n",
        "x.grad == u"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE4f8fDv4xIK",
        "outputId": "20840269-5984-4a4b-ba40-6f4470acc24b"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.grad.zero_()\n",
        "y.sum().backward()\n",
        "x.grad == 2 * x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6wpV3Qm4zt6",
        "outputId": "a6b1cbf8-6a9e-4de6-ec8c-205d7afbebc0"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([True, True, True, True])"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Computing the Gradient of Python Control Flow**"
      ],
      "metadata": {
        "id": "-wTyGkXw41Vs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One benefit of using automatic differentiation is that even if building the computational graph of a function required passing through a maze of Python control flow (e.g., conditionals, loops, and arbitrary function calls), we can still calculate the gradient of the resulting variable. In the following snippet, note that the number of iterations of the while loop and the evaluation of the if statement both depend on the value of the input a."
      ],
      "metadata": {
        "id": "pzgQIGbu4282"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(a):\n",
        "    b = a * 2\n",
        "    while b.norm() < 1000:\n",
        "        b = b * 2\n",
        "    if b.sum() > 0:\n",
        "        c = b\n",
        "    else:\n",
        "        c = 100 * b\n",
        "    return c"
      ],
      "metadata": {
        "id": "ELALaTij44Wa"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.randn(size=(), requires_grad=True)\n",
        "d = f(a)\n",
        "d.backward()"
      ],
      "metadata": {
        "id": "rmSVLyo_46KH"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a.grad == d / a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkrOZ7cZ48gM",
        "outputId": "e0fc8a4a-6ee7-403b-c6fc-5cb544cfca4d"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(True)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**"
      ],
      "metadata": {
        "id": "06RQfOg14-wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Deep learning frameworks can automate the calculation of derivatives. To use it, we first attach gradients to those variables with respect to which we desire partial derivatives. We then record the computation of our target value, execute its function for backpropagation, and access the resulting gradient"
      ],
      "metadata": {
        "id": "tRM-W5SC5ANB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Probability**"
      ],
      "metadata": {
        "id": "oDTV_JJk5BCh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In some form or another, machine learning is all about making predictions. We might want to predict the probability of a patient suffering a heart attack in the next year, given their clinical history. In anomaly detection, we might want to assess how likely a set of readings from an airplane‚Äôs jet engine would be, were it operating normally. In reinforcement learning, we want an agent to act intelligently in an environment. This means we need to think about the probability of getting a high reward under each of the available actions. And when we build recommender systems we also need to think about probability. For example, say hypothetically that we worked for a large online bookseller. We might want to estimate the probability that a particular user would buy a particular book. For this we need to use the language of probability. Entire courses, majors, theses, careers, and even departments, are devoted to probability. So naturally, our goal in this section is not to teach the whole subject. Instead we hope to get you off the ground, to teach you just enough that you can start building your first deep learning models, and to give you enough of a flavor for the subject that you can begin to explore it on your own if you wish."
      ],
      "metadata": {
        "id": "0o0c3has5G-3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Basic Probability Theory**"
      ],
      "metadata": {
        "id": "nRk9Q5zf5KaP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Say that we cast a die and want to know what the chance is of seeing a 1 rather than another digit. If the die is fair, all the six outcomes {1,‚Ä¶,6} are equally likely to occur, and thus we would see a 1 in one out of six cases. Formally we state that 1 occurs with probability 16.\n",
        "\n",
        "For a real die that we receive from a factory, we might not know those proportions and we would need to check whether it is tainted. The only way to investigate the die is by casting it many times and recording the outcomes. For each cast of the die, we will observe a value in {1,‚Ä¶,6}.\n",
        "\n",
        "Given these outcomes, we want to investigate the probability of observing each outcome.\n",
        "\n",
        "One natural approach for each value is to take the individual count for that value and to divide it by the total number of tosses. This gives us an estimate of the probability of a given event. The law of large numbers tell us that as the number of tosses grows this estimate will draw closer and closer to the true underlying probability. Before going into the details of what is going here, let us try it out."
      ],
      "metadata": {
        "id": "xCjIsXjH5Ma1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.distributions import multinomial\n",
        "from d2l import torch as d2l"
      ],
      "metadata": {
        "id": "TgfxO4r15TYN"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fair_probs = torch.ones([6]) / 6\n",
        "multinomial.Multinomial(1, fair_probs).sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "531FH6e75VBM",
        "outputId": "c388e5a3-2728-4214-d0b3-8a7df0cd7bbd"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0., 0., 0., 1., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "multinomial.Multinomial(10, fair_probs).sample()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Z12oCZ5XHJ",
        "outputId": "0369ab18-c48f-4529-ba43-7afbb851a248"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 4., 2., 2., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the results as 32-bit floats for division\n",
        "counts = multinomial.Multinomial(1000, fair_probs).sample()\n",
        "counts / 1000  # Relative frequency as the estimate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ASflAJHn5Y22",
        "outputId": "8cb4a1cd-d5d7-4b7a-d673-c2cf126caf94"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1620, 0.1640, 0.1770, 0.1790, 0.1710, 0.1470])"
            ]
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = multinomial.Multinomial(10, fair_probs).sample((500,))\n",
        "cum_counts = counts.cumsum(dim=0)\n",
        "estimates = cum_counts / cum_counts.sum(dim=1, keepdims=True)\n",
        "\n",
        "d2l.set_figsize((6, 4.5))\n",
        "for i in range(6):\n",
        "    d2l.plt.plot(estimates[:, i].numpy(),\n",
        "                 label=(\"P(die=\" + str(i + 1) + \")\"))\n",
        "d2l.plt.axhline(y=0.167, color='black', linestyle='dashed')\n",
        "d2l.plt.gca().set_xlabel('Groups of experiments')\n",
        "d2l.plt.gca().set_ylabel('Estimated probability')\n",
        "d2l.plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "rA1Pf7hv5aiy",
        "outputId": "98fdfac6-6678-4b45-e213-22735834c165"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x324 with 1 Axes>"
            ],
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<!-- Created with matplotlib (https://matplotlib.org/) -->\n<svg height=\"289.37625pt\" version=\"1.1\" viewBox=\"0 0 392.14375 289.37625\" width=\"392.14375pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-21T22:07:56.751252</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.3.3, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 289.37625 \nL 392.14375 289.37625 \nL 392.14375 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \nL 384.94375 7.2 \nL 50.14375 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md410e11ad4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"65.361932\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(62.180682 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 31.78125 66.40625 \nQ 24.171875 66.40625 20.328125 58.90625 \nQ 16.5 51.421875 16.5 36.375 \nQ 16.5 21.390625 20.328125 13.890625 \nQ 24.171875 6.390625 31.78125 6.390625 \nQ 39.453125 6.390625 43.28125 13.890625 \nQ 47.125 21.390625 47.125 36.375 \nQ 47.125 51.421875 43.28125 58.90625 \nQ 39.453125 66.40625 31.78125 66.40625 \nz\nM 31.78125 74.21875 \nQ 44.046875 74.21875 50.515625 64.515625 \nQ 56.984375 54.828125 56.984375 36.375 \nQ 56.984375 17.96875 50.515625 8.265625 \nQ 44.046875 -1.421875 31.78125 -1.421875 \nQ 19.53125 -1.421875 13.0625 8.265625 \nQ 6.59375 17.96875 6.59375 36.375 \nQ 6.59375 54.828125 13.0625 64.515625 \nQ 19.53125 74.21875 31.78125 74.21875 \nz\n\" id=\"DejaVuSans-48\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"126.356649\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 100 -->\n      <g transform=\"translate(116.812899 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 12.40625 8.296875 \nL 28.515625 8.296875 \nL 28.515625 63.921875 \nL 10.984375 60.40625 \nL 10.984375 69.390625 \nL 28.421875 72.90625 \nL 38.28125 72.90625 \nL 38.28125 8.296875 \nL 54.390625 8.296875 \nL 54.390625 0 \nL 12.40625 0 \nz\n\" id=\"DejaVuSans-49\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"187.351365\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 200 -->\n      <g transform=\"translate(177.807615 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 19.1875 8.296875 \nL 53.609375 8.296875 \nL 53.609375 0 \nL 7.328125 0 \nL 7.328125 8.296875 \nQ 12.9375 14.109375 22.625 23.890625 \nQ 32.328125 33.6875 34.8125 36.53125 \nQ 39.546875 41.84375 41.421875 45.53125 \nQ 43.3125 49.21875 43.3125 52.78125 \nQ 43.3125 58.59375 39.234375 62.25 \nQ 35.15625 65.921875 28.609375 65.921875 \nQ 23.96875 65.921875 18.8125 64.3125 \nQ 13.671875 62.703125 7.8125 59.421875 \nL 7.8125 69.390625 \nQ 13.765625 71.78125 18.9375 73 \nQ 24.125 74.21875 28.421875 74.21875 \nQ 39.75 74.21875 46.484375 68.546875 \nQ 53.21875 62.890625 53.21875 53.421875 \nQ 53.21875 48.921875 51.53125 44.890625 \nQ 49.859375 40.875 45.40625 35.40625 \nQ 44.1875 33.984375 37.640625 27.21875 \nQ 31.109375 20.453125 19.1875 8.296875 \nz\n\" id=\"DejaVuSans-50\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"248.346082\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 300 -->\n      <g transform=\"translate(238.802332 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 40.578125 39.3125 \nQ 47.65625 37.796875 51.625 33 \nQ 55.609375 28.21875 55.609375 21.1875 \nQ 55.609375 10.40625 48.1875 4.484375 \nQ 40.765625 -1.421875 27.09375 -1.421875 \nQ 22.515625 -1.421875 17.65625 -0.515625 \nQ 12.796875 0.390625 7.625 2.203125 \nL 7.625 11.71875 \nQ 11.71875 9.328125 16.59375 8.109375 \nQ 21.484375 6.890625 26.8125 6.890625 \nQ 36.078125 6.890625 40.9375 10.546875 \nQ 45.796875 14.203125 45.796875 21.1875 \nQ 45.796875 27.640625 41.28125 31.265625 \nQ 36.765625 34.90625 28.71875 34.90625 \nL 20.21875 34.90625 \nL 20.21875 43.015625 \nL 29.109375 43.015625 \nQ 36.375 43.015625 40.234375 45.921875 \nQ 44.09375 48.828125 44.09375 54.296875 \nQ 44.09375 59.90625 40.109375 62.90625 \nQ 36.140625 65.921875 28.71875 65.921875 \nQ 24.65625 65.921875 20.015625 65.03125 \nQ 15.375 64.15625 9.8125 62.3125 \nL 9.8125 71.09375 \nQ 15.4375 72.65625 20.34375 73.4375 \nQ 25.25 74.21875 29.59375 74.21875 \nQ 40.828125 74.21875 47.359375 69.109375 \nQ 53.90625 64.015625 53.90625 55.328125 \nQ 53.90625 49.265625 50.4375 45.09375 \nQ 46.96875 40.921875 40.578125 39.3125 \nz\n\" id=\"DejaVuSans-51\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"309.340799\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 400 -->\n      <g transform=\"translate(299.797049 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 37.796875 64.3125 \nL 12.890625 25.390625 \nL 37.796875 25.390625 \nz\nM 35.203125 72.90625 \nL 47.609375 72.90625 \nL 47.609375 25.390625 \nL 58.015625 25.390625 \nL 58.015625 17.1875 \nL 47.609375 17.1875 \nL 47.609375 0 \nL 37.796875 0 \nL 37.796875 17.1875 \nL 4.890625 17.1875 \nL 4.890625 26.703125 \nz\n\" id=\"DejaVuSans-52\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-52\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"370.335515\" xlink:href=\"#md410e11ad4\" y=\"251.82\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 500 -->\n      <g transform=\"translate(360.791765 266.418437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.796875 72.90625 \nL 49.515625 72.90625 \nL 49.515625 64.59375 \nL 19.828125 64.59375 \nL 19.828125 46.734375 \nQ 21.96875 47.46875 24.109375 47.828125 \nQ 26.265625 48.1875 28.421875 48.1875 \nQ 40.625 48.1875 47.75 41.5 \nQ 54.890625 34.8125 54.890625 23.390625 \nQ 54.890625 11.625 47.5625 5.09375 \nQ 40.234375 -1.421875 26.90625 -1.421875 \nQ 22.3125 -1.421875 17.546875 -0.640625 \nQ 12.796875 0.140625 7.71875 1.703125 \nL 7.71875 11.625 \nQ 12.109375 9.234375 16.796875 8.0625 \nQ 21.484375 6.890625 26.703125 6.890625 \nQ 35.15625 6.890625 40.078125 11.328125 \nQ 45.015625 15.765625 45.015625 23.390625 \nQ 45.015625 31 40.078125 35.4375 \nQ 35.15625 39.890625 26.703125 39.890625 \nQ 22.75 39.890625 18.8125 39.015625 \nQ 14.890625 38.140625 10.796875 36.28125 \nz\n\" id=\"DejaVuSans-53\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-53\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- Groups of experiments -->\n     <g transform=\"translate(160.397656 280.096562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 59.515625 10.40625 \nL 59.515625 29.984375 \nL 43.40625 29.984375 \nL 43.40625 38.09375 \nL 69.28125 38.09375 \nL 69.28125 6.78125 \nQ 63.578125 2.734375 56.6875 0.65625 \nQ 49.8125 -1.421875 42 -1.421875 \nQ 24.90625 -1.421875 15.25 8.5625 \nQ 5.609375 18.5625 5.609375 36.375 \nQ 5.609375 54.25 15.25 64.234375 \nQ 24.90625 74.21875 42 74.21875 \nQ 49.125 74.21875 55.546875 72.453125 \nQ 61.96875 70.703125 67.390625 67.28125 \nL 67.390625 56.78125 \nQ 61.921875 61.421875 55.765625 63.765625 \nQ 49.609375 66.109375 42.828125 66.109375 \nQ 29.4375 66.109375 22.71875 58.640625 \nQ 16.015625 51.171875 16.015625 36.375 \nQ 16.015625 21.625 22.71875 14.15625 \nQ 29.4375 6.6875 42.828125 6.6875 \nQ 48.046875 6.6875 52.140625 7.59375 \nQ 56.25 8.5 59.515625 10.40625 \nz\n\" id=\"DejaVuSans-71\"/>\n       <path d=\"M 41.109375 46.296875 \nQ 39.59375 47.171875 37.8125 47.578125 \nQ 36.03125 48 33.890625 48 \nQ 26.265625 48 22.1875 43.046875 \nQ 18.109375 38.09375 18.109375 28.8125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 20.953125 51.171875 25.484375 53.578125 \nQ 30.03125 56 36.53125 56 \nQ 37.453125 56 38.578125 55.875 \nQ 39.703125 55.765625 41.0625 55.515625 \nz\n\" id=\"DejaVuSans-114\"/>\n       <path d=\"M 30.609375 48.390625 \nQ 23.390625 48.390625 19.1875 42.75 \nQ 14.984375 37.109375 14.984375 27.296875 \nQ 14.984375 17.484375 19.15625 11.84375 \nQ 23.34375 6.203125 30.609375 6.203125 \nQ 37.796875 6.203125 41.984375 11.859375 \nQ 46.1875 17.53125 46.1875 27.296875 \nQ 46.1875 37.015625 41.984375 42.703125 \nQ 37.796875 48.390625 30.609375 48.390625 \nz\nM 30.609375 56 \nQ 42.328125 56 49.015625 48.375 \nQ 55.71875 40.765625 55.71875 27.296875 \nQ 55.71875 13.875 49.015625 6.21875 \nQ 42.328125 -1.421875 30.609375 -1.421875 \nQ 18.84375 -1.421875 12.171875 6.21875 \nQ 5.515625 13.875 5.515625 27.296875 \nQ 5.515625 40.765625 12.171875 48.375 \nQ 18.84375 56 30.609375 56 \nz\n\" id=\"DejaVuSans-111\"/>\n       <path d=\"M 8.5 21.578125 \nL 8.5 54.6875 \nL 17.484375 54.6875 \nL 17.484375 21.921875 \nQ 17.484375 14.15625 20.5 10.265625 \nQ 23.53125 6.390625 29.59375 6.390625 \nQ 36.859375 6.390625 41.078125 11.03125 \nQ 45.3125 15.671875 45.3125 23.6875 \nL 45.3125 54.6875 \nL 54.296875 54.6875 \nL 54.296875 0 \nL 45.3125 0 \nL 45.3125 8.40625 \nQ 42.046875 3.421875 37.71875 1 \nQ 33.40625 -1.421875 27.6875 -1.421875 \nQ 18.265625 -1.421875 13.375 4.4375 \nQ 8.5 10.296875 8.5 21.578125 \nz\nM 31.109375 56 \nz\n\" id=\"DejaVuSans-117\"/>\n       <path d=\"M 18.109375 8.203125 \nL 18.109375 -20.796875 \nL 9.078125 -20.796875 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nz\nM 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\n\" id=\"DejaVuSans-112\"/>\n       <path d=\"M 44.28125 53.078125 \nL 44.28125 44.578125 \nQ 40.484375 46.53125 36.375 47.5 \nQ 32.28125 48.484375 27.875 48.484375 \nQ 21.1875 48.484375 17.84375 46.4375 \nQ 14.5 44.390625 14.5 40.28125 \nQ 14.5 37.15625 16.890625 35.375 \nQ 19.28125 33.59375 26.515625 31.984375 \nL 29.59375 31.296875 \nQ 39.15625 29.25 43.1875 25.515625 \nQ 47.21875 21.78125 47.21875 15.09375 \nQ 47.21875 7.46875 41.1875 3.015625 \nQ 35.15625 -1.421875 24.609375 -1.421875 \nQ 20.21875 -1.421875 15.453125 -0.5625 \nQ 10.6875 0.296875 5.421875 2 \nL 5.421875 11.28125 \nQ 10.40625 8.6875 15.234375 7.390625 \nQ 20.0625 6.109375 24.8125 6.109375 \nQ 31.15625 6.109375 34.5625 8.28125 \nQ 37.984375 10.453125 37.984375 14.40625 \nQ 37.984375 18.0625 35.515625 20.015625 \nQ 33.0625 21.96875 24.703125 23.78125 \nL 21.578125 24.515625 \nQ 13.234375 26.265625 9.515625 29.90625 \nQ 5.8125 33.546875 5.8125 39.890625 \nQ 5.8125 47.609375 11.28125 51.796875 \nQ 16.75 56 26.8125 56 \nQ 31.78125 56 36.171875 55.265625 \nQ 40.578125 54.546875 44.28125 53.078125 \nz\n\" id=\"DejaVuSans-115\"/>\n       <path id=\"DejaVuSans-32\"/>\n       <path d=\"M 37.109375 75.984375 \nL 37.109375 68.5 \nL 28.515625 68.5 \nQ 23.6875 68.5 21.796875 66.546875 \nQ 19.921875 64.59375 19.921875 59.515625 \nL 19.921875 54.6875 \nL 34.71875 54.6875 \nL 34.71875 47.703125 \nL 19.921875 47.703125 \nL 19.921875 0 \nL 10.890625 0 \nL 10.890625 47.703125 \nL 2.296875 47.703125 \nL 2.296875 54.6875 \nL 10.890625 54.6875 \nL 10.890625 58.5 \nQ 10.890625 67.625 15.140625 71.796875 \nQ 19.390625 75.984375 28.609375 75.984375 \nz\n\" id=\"DejaVuSans-102\"/>\n       <path d=\"M 56.203125 29.59375 \nL 56.203125 25.203125 \nL 14.890625 25.203125 \nQ 15.484375 15.921875 20.484375 11.0625 \nQ 25.484375 6.203125 34.421875 6.203125 \nQ 39.59375 6.203125 44.453125 7.46875 \nQ 49.3125 8.734375 54.109375 11.28125 \nL 54.109375 2.78125 \nQ 49.265625 0.734375 44.1875 -0.34375 \nQ 39.109375 -1.421875 33.890625 -1.421875 \nQ 20.796875 -1.421875 13.15625 6.1875 \nQ 5.515625 13.8125 5.515625 26.8125 \nQ 5.515625 40.234375 12.765625 48.109375 \nQ 20.015625 56 32.328125 56 \nQ 43.359375 56 49.78125 48.890625 \nQ 56.203125 41.796875 56.203125 29.59375 \nz\nM 47.21875 32.234375 \nQ 47.125 39.59375 43.09375 43.984375 \nQ 39.0625 48.390625 32.421875 48.390625 \nQ 24.90625 48.390625 20.390625 44.140625 \nQ 15.875 39.890625 15.1875 32.171875 \nz\n\" id=\"DejaVuSans-101\"/>\n       <path d=\"M 54.890625 54.6875 \nL 35.109375 28.078125 \nL 55.90625 0 \nL 45.3125 0 \nL 29.390625 21.484375 \nL 13.484375 0 \nL 2.875 0 \nL 24.125 28.609375 \nL 4.6875 54.6875 \nL 15.28125 54.6875 \nL 29.78125 35.203125 \nL 44.28125 54.6875 \nz\n\" id=\"DejaVuSans-120\"/>\n       <path d=\"M 9.421875 54.6875 \nL 18.40625 54.6875 \nL 18.40625 0 \nL 9.421875 0 \nz\nM 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 64.59375 \nL 9.421875 64.59375 \nz\n\" id=\"DejaVuSans-105\"/>\n       <path d=\"M 52 44.1875 \nQ 55.375 50.25 60.0625 53.125 \nQ 64.75 56 71.09375 56 \nQ 79.640625 56 84.28125 50.015625 \nQ 88.921875 44.046875 88.921875 33.015625 \nL 88.921875 0 \nL 79.890625 0 \nL 79.890625 32.71875 \nQ 79.890625 40.578125 77.09375 44.375 \nQ 74.3125 48.1875 68.609375 48.1875 \nQ 61.625 48.1875 57.5625 43.546875 \nQ 53.515625 38.921875 53.515625 30.90625 \nL 53.515625 0 \nL 44.484375 0 \nL 44.484375 32.71875 \nQ 44.484375 40.625 41.703125 44.40625 \nQ 38.921875 48.1875 33.109375 48.1875 \nQ 26.21875 48.1875 22.15625 43.53125 \nQ 18.109375 38.875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.1875 51.21875 25.484375 53.609375 \nQ 29.78125 56 35.6875 56 \nQ 41.65625 56 45.828125 52.96875 \nQ 50 49.953125 52 44.1875 \nz\n\" id=\"DejaVuSans-109\"/>\n       <path d=\"M 54.890625 33.015625 \nL 54.890625 0 \nL 45.90625 0 \nL 45.90625 32.71875 \nQ 45.90625 40.484375 42.875 44.328125 \nQ 39.84375 48.1875 33.796875 48.1875 \nQ 26.515625 48.1875 22.3125 43.546875 \nQ 18.109375 38.921875 18.109375 30.90625 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 54.6875 \nL 18.109375 54.6875 \nL 18.109375 46.1875 \nQ 21.34375 51.125 25.703125 53.5625 \nQ 30.078125 56 35.796875 56 \nQ 45.21875 56 50.046875 50.171875 \nQ 54.890625 44.34375 54.890625 33.015625 \nz\n\" id=\"DejaVuSans-110\"/>\n       <path d=\"M 18.3125 70.21875 \nL 18.3125 54.6875 \nL 36.8125 54.6875 \nL 36.8125 47.703125 \nL 18.3125 47.703125 \nL 18.3125 18.015625 \nQ 18.3125 11.328125 20.140625 9.421875 \nQ 21.96875 7.515625 27.59375 7.515625 \nL 36.8125 7.515625 \nL 36.8125 0 \nL 27.59375 0 \nQ 17.1875 0 13.234375 3.875 \nQ 9.28125 7.765625 9.28125 18.015625 \nL 9.28125 47.703125 \nL 2.6875 47.703125 \nL 2.6875 54.6875 \nL 9.28125 54.6875 \nL 9.28125 70.21875 \nz\n\" id=\"DejaVuSans-116\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-71\"/>\n      <use x=\"77.490234\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"116.353516\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"177.535156\" xlink:href=\"#DejaVuSans-117\"/>\n      <use x=\"240.914062\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"304.390625\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"356.490234\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"388.277344\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"449.458984\" xlink:href=\"#DejaVuSans-102\"/>\n      <use x=\"484.664062\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"516.451172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"576.224609\" xlink:href=\"#DejaVuSans-120\"/>\n      <use x=\"635.404297\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"698.880859\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"760.404297\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"801.517578\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"829.300781\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"926.712891\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"988.236328\" xlink:href=\"#DejaVuSans-110\"/>\n      <use x=\"1051.615234\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1090.824219\" xlink:href=\"#DejaVuSans-115\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7c7f5d2e12\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"240.700909\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 0.00 -->\n      <g transform=\"translate(20.878125 244.500128)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 10.6875 12.40625 \nL 21 12.40625 \nL 21 0 \nL 10.6875 0 \nz\n\" id=\"DejaVuSans-46\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"203.637274\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 0.05 -->\n      <g transform=\"translate(20.878125 207.436493)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"166.573639\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 0.10 -->\n      <g transform=\"translate(20.878125 170.372858)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"129.510004\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 0.15 -->\n      <g transform=\"translate(20.878125 133.309223)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-49\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"92.44637\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 0.20 -->\n      <g transform=\"translate(20.878125 96.245588)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"55.382735\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- 0.25 -->\n      <g transform=\"translate(20.878125 59.181953)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-53\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"50.14375\" xlink:href=\"#m7c7f5d2e12\" y=\"18.3191\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- 0.30 -->\n      <g transform=\"translate(20.878125 22.118318)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-48\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-51\"/>\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-48\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- Estimated probability -->\n     <g transform=\"translate(14.798438 183.033437)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 9.8125 72.90625 \nL 55.90625 72.90625 \nL 55.90625 64.59375 \nL 19.671875 64.59375 \nL 19.671875 43.015625 \nL 54.390625 43.015625 \nL 54.390625 34.71875 \nL 19.671875 34.71875 \nL 19.671875 8.296875 \nL 56.78125 8.296875 \nL 56.78125 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-69\"/>\n       <path d=\"M 34.28125 27.484375 \nQ 23.390625 27.484375 19.1875 25 \nQ 14.984375 22.515625 14.984375 16.5 \nQ 14.984375 11.71875 18.140625 8.90625 \nQ 21.296875 6.109375 26.703125 6.109375 \nQ 34.1875 6.109375 38.703125 11.40625 \nQ 43.21875 16.703125 43.21875 25.484375 \nL 43.21875 27.484375 \nz\nM 52.203125 31.203125 \nL 52.203125 0 \nL 43.21875 0 \nL 43.21875 8.296875 \nQ 40.140625 3.328125 35.546875 0.953125 \nQ 30.953125 -1.421875 24.3125 -1.421875 \nQ 15.921875 -1.421875 10.953125 3.296875 \nQ 6 8.015625 6 15.921875 \nQ 6 25.140625 12.171875 29.828125 \nQ 18.359375 34.515625 30.609375 34.515625 \nL 43.21875 34.515625 \nL 43.21875 35.40625 \nQ 43.21875 41.609375 39.140625 45 \nQ 35.0625 48.390625 27.6875 48.390625 \nQ 23 48.390625 18.546875 47.265625 \nQ 14.109375 46.140625 10.015625 43.890625 \nL 10.015625 52.203125 \nQ 14.9375 54.109375 19.578125 55.046875 \nQ 24.21875 56 28.609375 56 \nQ 40.484375 56 46.34375 49.84375 \nQ 52.203125 43.703125 52.203125 31.203125 \nz\n\" id=\"DejaVuSans-97\"/>\n       <path d=\"M 45.40625 46.390625 \nL 45.40625 75.984375 \nL 54.390625 75.984375 \nL 54.390625 0 \nL 45.40625 0 \nL 45.40625 8.203125 \nQ 42.578125 3.328125 38.25 0.953125 \nQ 33.9375 -1.421875 27.875 -1.421875 \nQ 17.96875 -1.421875 11.734375 6.484375 \nQ 5.515625 14.40625 5.515625 27.296875 \nQ 5.515625 40.1875 11.734375 48.09375 \nQ 17.96875 56 27.875 56 \nQ 33.9375 56 38.25 53.625 \nQ 42.578125 51.265625 45.40625 46.390625 \nz\nM 14.796875 27.296875 \nQ 14.796875 17.390625 18.875 11.75 \nQ 22.953125 6.109375 30.078125 6.109375 \nQ 37.203125 6.109375 41.296875 11.75 \nQ 45.40625 17.390625 45.40625 27.296875 \nQ 45.40625 37.203125 41.296875 42.84375 \nQ 37.203125 48.484375 30.078125 48.484375 \nQ 22.953125 48.484375 18.875 42.84375 \nQ 14.796875 37.203125 14.796875 27.296875 \nz\n\" id=\"DejaVuSans-100\"/>\n       <path d=\"M 48.6875 27.296875 \nQ 48.6875 37.203125 44.609375 42.84375 \nQ 40.53125 48.484375 33.40625 48.484375 \nQ 26.265625 48.484375 22.1875 42.84375 \nQ 18.109375 37.203125 18.109375 27.296875 \nQ 18.109375 17.390625 22.1875 11.75 \nQ 26.265625 6.109375 33.40625 6.109375 \nQ 40.53125 6.109375 44.609375 11.75 \nQ 48.6875 17.390625 48.6875 27.296875 \nz\nM 18.109375 46.390625 \nQ 20.953125 51.265625 25.265625 53.625 \nQ 29.59375 56 35.59375 56 \nQ 45.5625 56 51.78125 48.09375 \nQ 58.015625 40.1875 58.015625 27.296875 \nQ 58.015625 14.40625 51.78125 6.484375 \nQ 45.5625 -1.421875 35.59375 -1.421875 \nQ 29.59375 -1.421875 25.265625 0.953125 \nQ 20.953125 3.328125 18.109375 8.203125 \nL 18.109375 0 \nL 9.078125 0 \nL 9.078125 75.984375 \nL 18.109375 75.984375 \nz\n\" id=\"DejaVuSans-98\"/>\n       <path d=\"M 9.421875 75.984375 \nL 18.40625 75.984375 \nL 18.40625 0 \nL 9.421875 0 \nz\n\" id=\"DejaVuSans-108\"/>\n       <path d=\"M 32.171875 -5.078125 \nQ 28.375 -14.84375 24.75 -17.8125 \nQ 21.140625 -20.796875 15.09375 -20.796875 \nL 7.90625 -20.796875 \nL 7.90625 -13.28125 \nL 13.1875 -13.28125 \nQ 16.890625 -13.28125 18.9375 -11.515625 \nQ 21 -9.765625 23.484375 -3.21875 \nL 25.09375 0.875 \nL 2.984375 54.6875 \nL 12.5 54.6875 \nL 29.59375 11.921875 \nL 46.6875 54.6875 \nL 56.203125 54.6875 \nz\n\" id=\"DejaVuSans-121\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"63.183594\" xlink:href=\"#DejaVuSans-115\"/>\n      <use x=\"115.283203\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"154.492188\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"182.275391\" xlink:href=\"#DejaVuSans-109\"/>\n      <use x=\"279.6875\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"340.966797\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"380.175781\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"441.699219\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"505.175781\" xlink:href=\"#DejaVuSans-32\"/>\n      <use x=\"536.962891\" xlink:href=\"#DejaVuSans-112\"/>\n      <use x=\"600.439453\" xlink:href=\"#DejaVuSans-114\"/>\n      <use x=\"639.302734\" xlink:href=\"#DejaVuSans-111\"/>\n      <use x=\"700.484375\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"763.960938\" xlink:href=\"#DejaVuSans-97\"/>\n      <use x=\"825.240234\" xlink:href=\"#DejaVuSans-98\"/>\n      <use x=\"888.716797\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"916.5\" xlink:href=\"#DejaVuSans-108\"/>\n      <use x=\"944.283203\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"972.066406\" xlink:href=\"#DejaVuSans-116\"/>\n      <use x=\"1011.275391\" xlink:href=\"#DejaVuSans-121\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"line2d_14\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 92.446367 \nL 65.971879 92.446367 \nL 66.581826 141.864544 \nL 67.191773 148.041822 \nL 67.80172 151.748187 \nL 68.411668 129.51 \nL 69.631562 73.914557 \nL 70.241509 92.446367 \nL 70.851456 92.446367 \nL 71.461403 105.924051 \nL 72.071351 104.800912 \nL 73.291245 124.215195 \nL 73.901192 117.155456 \nL 75.121086 114.248504 \nL 75.731034 113.037274 \nL 76.340981 119.756416 \nL 76.950928 122.09728 \nL 77.560875 117.155456 \nL 78.170822 116.032316 \nL 79.390717 120.2441 \nL 80.000664 122.09728 \nL 81.220558 130.882731 \nL 81.830505 134.804805 \nL 83.660347 130.705611 \nL 84.270294 127.193525 \nL 85.490188 129.51 \nL 87.32003 126.50484 \nL 87.929977 121.707131 \nL 88.539924 122.857553 \nL 89.149871 122.09728 \nL 89.759819 117.758116 \nL 90.369766 120.685326 \nL 91.58966 122.771164 \nL 92.199607 122.09728 \nL 94.029449 124.87705 \nL 94.639396 124.215195 \nL 95.249343 126.544914 \nL 95.85929 124.422835 \nL 97.079185 123.216178 \nL 97.689132 124.019099 \nL 98.299079 126.140582 \nL 99.518973 127.559285 \nL 100.12892 126.953897 \nL 101.348815 123.332733 \nL 101.958762 120.395991 \nL 102.568709 119.9452 \nL 103.178656 121.861949 \nL 105.008498 123.894303 \nL 105.618445 125.637688 \nL 106.228392 124.059471 \nL 107.448286 123.156243 \nL 108.058234 123.767751 \nL 108.668181 121.273637 \nL 109.278128 121.894192 \nL 109.888075 121.496244 \nL 110.498022 123.085638 \nL 111.107969 123.657856 \nL 114.157705 121.731221 \nL 114.767652 120.470098 \nL 118.427335 118.55938 \nL 119.037283 119.931757 \nL 120.257177 120.956854 \nL 120.867124 119.035499 \nL 122.087018 120.046943 \nL 122.696966 121.316992 \nL 123.91686 117.664922 \nL 125.746701 119.132183 \nL 126.356649 118.867967 \nL 126.966596 120.062407 \nL 127.576543 119.794292 \nL 128.18649 120.2441 \nL 128.796437 119.979354 \nL 129.406384 120.418922 \nL 130.626279 119.900917 \nL 131.236226 120.329109 \nL 131.846173 119.401746 \nL 132.45612 120.494531 \nL 133.676015 119.998099 \nL 134.285962 119.106181 \nL 134.895909 118.874352 \nL 136.115803 119.689721 \nL 136.72575 119.458852 \nL 137.335698 119.854768 \nL 138.555592 121.852228 \nL 140.385433 122.9342 \nL 140.995381 123.876332 \nL 141.605328 123.626884 \nL 142.215275 123.965052 \nL 142.825222 124.87705 \nL 144.045116 125.518536 \nL 145.265011 122.771164 \nL 145.874958 123.100506 \nL 146.484905 121.765364 \nL 147.094852 121.548181 \nL 148.314747 122.205496 \nL 148.924694 121.989848 \nL 149.534641 122.310597 \nL 150.144588 122.09728 \nL 150.754535 122.412716 \nL 151.97443 121.993604 \nL 152.584377 122.303185 \nL 153.194324 121.074834 \nL 153.804271 121.894192 \nL 154.414218 122.198128 \nL 155.634113 123.788771 \nL 156.854007 123.373636 \nL 157.463954 123.657856 \nL 158.073901 124.422835 \nL 158.683848 124.69655 \nL 159.293796 124.01024 \nL 159.903743 124.283083 \nL 160.51369 125.0246 \nL 161.123637 124.818408 \nL 161.733584 125.547222 \nL 162.343531 125.340345 \nL 162.953479 125.596454 \nL 163.563426 125.391819 \nL 164.173373 125.64447 \nL 165.393267 125.24207 \nL 166.613162 125.737056 \nL 167.223109 125.097668 \nL 169.05295 124.524832 \nL 169.662897 123.476384 \nL 170.882792 123.119726 \nL 171.492739 123.79161 \nL 172.102686 123.613518 \nL 172.712633 123.856228 \nL 173.32258 123.679771 \nL 173.932528 123.091161 \nL 174.542475 123.744555 \nL 175.152422 123.162086 \nL 175.762369 123.40061 \nL 176.372316 123.231465 \nL 176.982263 122.661291 \nL 177.592211 122.497967 \nL 178.202158 123.133467 \nL 178.812105 122.969359 \nL 179.422052 122.412716 \nL 180.031999 122.646368 \nL 180.641946 123.267707 \nL 182.471788 121.636382 \nL 183.081735 121.868013 \nL 183.691682 121.717138 \nL 184.301629 121.945996 \nL 185.521524 121.648024 \nL 186.131471 121.873779 \nL 186.741418 121.356006 \nL 187.961312 121.803704 \nL 189.181207 121.515883 \nL 189.791154 121.735684 \nL 190.401101 121.593502 \nL 191.011048 122.168901 \nL 191.620995 122.382384 \nL 192.230943 122.239153 \nL 192.84089 122.803252 \nL 193.450837 123.010692 \nL 194.060784 123.565833 \nL 194.670731 123.767751 \nL 195.280678 123.274997 \nL 195.890626 123.131611 \nL 196.500573 123.332733 \nL 198.940361 122.771164 \nL 199.550309 122.969359 \nL 200.160256 122.164063 \nL 201.38015 122.560575 \nL 201.990097 122.09728 \nL 203.209992 122.489142 \nL 204.429886 122.226759 \nL 205.039833 122.419575 \nL 205.64978 122.931615 \nL 206.259727 123.119726 \nL 206.869675 123.624365 \nL 207.479622 123.807903 \nL 208.699516 122.913931 \nL 209.309463 122.78538 \nL 209.91941 122.969359 \nL 210.529358 122.841647 \nL 211.139305 123.02387 \nL 211.749252 122.896987 \nL 212.359199 123.077475 \nL 212.969146 122.951421 \nL 213.579093 123.130197 \nL 214.189041 123.00496 \nL 214.798988 123.182068 \nL 215.408935 123.657856 \nL 216.018882 123.532 \nL 216.628829 124.002552 \nL 217.848724 124.341769 \nL 218.458671 124.215195 \nL 219.068618 123.503645 \nL 219.678565 123.089538 \nL 220.288512 122.969359 \nL 220.898459 122.560575 \nL 221.508407 121.866533 \nL 222.118354 122.03982 \nL 222.728301 121.925551 \nL 223.338248 121.527073 \nL 223.948195 121.415642 \nL 225.16809 121.759057 \nL 226.387984 121.537831 \nL 226.997931 120.871116 \nL 227.607878 120.48702 \nL 228.217825 120.658992 \nL 229.43772 119.900917 \nL 230.047667 120.073133 \nL 230.657614 119.42651 \nL 232.487456 119.132183 \nL 233.097403 118.229773 \nL 234.317297 118.577561 \nL 234.927244 117.952522 \nL 235.537191 117.861439 \nL 236.147139 117.507188 \nL 236.757086 117.681182 \nL 237.367033 117.59202 \nL 237.97698 118.025502 \nL 239.196874 118.364996 \nL 240.416769 118.185004 \nL 242.856557 118.847864 \nL 243.466505 118.251765 \nL 245.296346 118.741515 \nL 245.906293 118.652982 \nL 246.51624 118.316295 \nL 247.126188 118.229773 \nL 248.346082 117.565908 \nL 248.956029 117.973642 \nL 249.565976 117.644753 \nL 250.175923 117.805701 \nL 250.785871 117.236477 \nL 251.395818 116.91321 \nL 253.225659 116.675669 \nL 253.835606 116.836629 \nL 254.445554 117.234908 \nL 255.055501 117.393051 \nL 256.275395 116.762004 \nL 256.885342 116.920136 \nL 257.495289 116.608102 \nL 258.105237 116.064206 \nL 258.715184 116.456145 \nL 259.325131 116.148507 \nL 259.935078 116.074434 \nL 261.76492 116.543473 \nL 262.374867 116.011517 \nL 262.984814 115.939012 \nL 263.594761 116.094328 \nL 264.204708 116.475398 \nL 264.814655 116.628128 \nL 265.424603 116.329317 \nL 266.03455 116.7062 \nL 267.254444 116.113514 \nL 267.864391 116.042434 \nL 268.474338 115.749853 \nL 269.084286 115.901567 \nL 270.30418 116.642212 \nL 271.524074 116.936793 \nL 273.353916 116.721962 \nL 274.57381 117.011805 \nL 275.183757 116.510877 \nL 275.793704 116.227087 \nL 276.403652 116.585801 \nL 277.013599 116.72944 \nL 277.623546 116.022662 \nL 278.233493 115.955305 \nL 278.84344 116.310704 \nL 279.453387 116.242905 \nL 280.063335 116.385485 \nL 280.673282 116.736664 \nL 281.283229 116.668236 \nL 282.503123 116.117259 \nL 283.723018 115.985394 \nL 284.332965 115.302276 \nL 284.942912 115.03363 \nL 285.552859 115.380778 \nL 286.162806 115.521801 \nL 286.772753 115.865701 \nL 287.382701 116.004625 \nL 288.602595 115.876239 \nL 289.822489 116.552796 \nL 291.652331 116.358389 \nL 292.262278 116.095554 \nL 295.921961 115.721156 \nL 296.531908 115.854976 \nL 297.141855 115.598978 \nL 297.751802 115.538369 \nL 298.36175 115.865171 \nL 299.581644 115.743512 \nL 300.191591 115.875201 \nL 300.801538 115.623113 \nL 301.411485 115.563377 \nL 302.021433 115.313399 \nL 303.851274 115.138389 \nL 305.071168 115.399479 \nL 305.681116 114.966041 \nL 308.120904 115.483416 \nL 308.730851 115.425823 \nL 309.340799 115.738232 \nL 309.950746 115.864685 \nL 310.560693 115.622638 \nL 311.17064 115.748759 \nL 311.780587 115.691221 \nL 313.000482 115.941255 \nL 313.610429 115.883673 \nL 314.220376 116.007607 \nL 315.44027 115.892951 \nL 316.050217 115.656118 \nL 316.660165 115.599928 \nL 317.880059 115.131099 \nL 319.099953 115.022297 \nL 319.7099 115.322965 \nL 320.319848 115.445286 \nL 320.929795 114.684549 \nL 321.539742 114.631728 \nL 322.149689 114.754812 \nL 322.759636 114.526837 \nL 323.369583 114.824411 \nL 325.199425 115.187996 \nL 325.809372 114.788468 \nL 326.419319 114.563597 \nL 327.639214 114.804938 \nL 328.859108 115.386445 \nL 329.469055 114.991988 \nL 330.688949 115.2286 \nL 331.298897 115.515725 \nL 331.908844 115.463059 \nL 332.518791 115.579482 \nL 333.128738 115.358433 \nL 333.738685 115.474569 \nL 334.348632 115.254757 \nL 334.95858 115.370605 \nL 335.568527 114.985062 \nL 336.178474 114.934417 \nL 336.788421 114.551591 \nL 337.398368 114.8338 \nL 338.008315 114.949295 \nL 339.22821 115.508192 \nL 339.838157 114.963964 \nL 341.058051 115.519094 \nL 341.667998 115.305004 \nL 342.277946 115.417683 \nL 342.887893 115.204741 \nL 343.49784 115.317143 \nL 344.107787 114.943507 \nL 346.547576 114.748725 \nL 347.157523 114.380358 \nL 347.76747 114.333082 \nL 348.377417 114.445429 \nL 348.987365 114.398219 \nL 350.207259 113.987624 \nL 350.817206 113.941695 \nL 351.427153 114.053678 \nL 352.647048 113.64802 \nL 353.256995 113.603196 \nL 354.476889 113.826233 \nL 355.086836 113.781321 \nL 356.306731 114.002204 \nL 356.916678 113.6477 \nL 357.526625 113.757959 \nL 358.136572 114.021877 \nL 358.746519 113.669526 \nL 359.356466 113.625586 \nL 359.966414 113.734984 \nL 360.576361 113.691088 \nL 361.186308 113.7999 \nL 361.796255 114.060482 \nL 363.016149 113.972082 \nL 364.236044 113.582452 \nL 364.845991 113.69016 \nL 365.455938 113.64707 \nL 366.675832 113.860917 \nL 367.895727 113.774738 \nL 368.505674 114.029609 \nL 369.725568 113.943275 \nL 369.725568 113.943275 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_15\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 18.319091 \nL 66.581826 92.446367 \nL 67.191773 92.446367 \nL 67.80172 107.271818 \nL 68.411668 92.446367 \nL 69.021615 103.035977 \nL 69.631562 120.2441 \nL 70.241509 125.391819 \nL 70.851456 129.51 \nL 71.461403 126.140582 \nL 72.071351 117.155456 \nL 72.681298 120.956854 \nL 73.291245 118.920391 \nL 73.901192 127.039093 \nL 75.121086 131.690214 \nL 76.340981 127.559285 \nL 76.950928 133.216371 \nL 77.560875 127.745065 \nL 78.170822 132.879429 \nL 78.780769 127.898547 \nL 79.390717 129.51 \nL 80.000664 133.957645 \nL 80.610611 132.361049 \nL 81.220558 125.391819 \nL 82.440452 123.119726 \nL 83.0504 119.626373 \nL 83.660347 118.749589 \nL 84.880241 121.648024 \nL 85.490188 120.789145 \nL 86.100135 117.861439 \nL 86.710083 119.214552 \nL 87.32003 122.497967 \nL 87.929977 121.707131 \nL 88.539924 119.056155 \nL 89.149871 120.2441 \nL 89.759819 119.566107 \nL 90.369766 120.685326 \nL 90.979713 120.028607 \nL 92.199607 122.09728 \nL 92.809554 119.841226 \nL 94.029449 124.87705 \nL 94.639396 124.215195 \nL 95.249343 122.09728 \nL 95.85929 121.515883 \nL 96.469237 122.382384 \nL 97.079185 121.817556 \nL 97.689132 122.646368 \nL 98.909026 118.920391 \nL 99.518973 121.056896 \nL 100.12892 119.285555 \nL 101.348815 118.390909 \nL 101.958762 119.180796 \nL 102.568709 117.553989 \nL 103.178656 117.155456 \nL 104.398551 114.114342 \nL 105.008498 114.909177 \nL 105.618445 113.467531 \nL 107.448286 112.566634 \nL 108.058234 111.239202 \nL 108.668181 110.978189 \nL 109.278128 108.693438 \nL 109.888075 108.473891 \nL 110.498022 109.248546 \nL 111.107969 108.052106 \nL 112.937811 110.274449 \nL 113.547758 107.271818 \nL 114.157705 108.919093 \nL 114.767652 109.622204 \nL 115.3776 111.201458 \nL 115.987547 110.978189 \nL 116.597494 109.015998 \nL 117.207441 107.961376 \nL 118.427335 109.29348 \nL 119.037283 108.271289 \nL 119.64723 108.095461 \nL 120.257177 106.294318 \nL 120.867124 105.33807 \nL 123.306913 107.889556 \nL 123.91686 109.258741 \nL 124.526807 108.330781 \nL 125.136754 108.919093 \nL 125.746701 107.271818 \nL 127.576543 108.999065 \nL 128.18649 108.839905 \nL 128.796437 109.389744 \nL 129.406384 109.2299 \nL 130.016332 110.458605 \nL 130.626279 110.978189 \nL 131.236226 112.168299 \nL 131.846173 111.315131 \nL 132.45612 111.812955 \nL 133.066067 111.640033 \nL 133.676015 112.782171 \nL 134.285962 111.303312 \nL 134.895909 111.139337 \nL 135.505856 112.256246 \nL 136.115803 111.453358 \nL 136.72575 112.548684 \nL 137.335698 113.002668 \nL 137.945645 111.595916 \nL 139.775486 111.128854 \nL 140.385433 112.173789 \nL 140.995381 112.015966 \nL 141.605328 110.684028 \nL 143.435169 113.707679 \nL 144.045116 113.544134 \nL 144.655064 112.817219 \nL 145.265011 113.224468 \nL 145.874958 113.068247 \nL 146.484905 114.020728 \nL 147.094852 113.860917 \nL 147.704799 112.613347 \nL 148.314747 111.92507 \nL 148.924694 111.783916 \nL 149.534641 108.978354 \nL 150.754535 107.692399 \nL 151.364482 108.107059 \nL 152.584377 106.860008 \nL 153.194324 106.760606 \nL 153.804271 107.677995 \nL 154.414218 107.070121 \nL 155.024165 107.472167 \nL 155.634113 107.371319 \nL 156.24406 107.766008 \nL 156.854007 107.664552 \nL 158.073901 105.527649 \nL 158.683848 104.961363 \nL 159.293796 105.358858 \nL 159.903743 105.276092 \nL 160.51369 106.138671 \nL 161.123637 105.113685 \nL 161.733584 105.500234 \nL 162.343531 105.418638 \nL 162.953479 104.877647 \nL 164.78332 107.362217 \nL 165.393267 107.271818 \nL 166.003214 107.629062 \nL 166.613162 107.094279 \nL 167.223109 106.124621 \nL 167.833056 106.482307 \nL 168.443003 105.963694 \nL 170.272845 105.729291 \nL 170.882792 106.078969 \nL 171.492739 106.001073 \nL 172.712633 104.172713 \nL 173.32258 104.523286 \nL 173.932528 104.455818 \nL 174.542475 104.800912 \nL 175.152422 104.323113 \nL 175.762369 104.257855 \nL 176.982263 104.935207 \nL 177.592211 105.669069 \nL 178.202158 105.996512 \nL 179.422052 105.852364 \nL 180.031999 104.997019 \nL 181.861841 105.959155 \nL 182.471788 105.889135 \nL 183.081735 105.05565 \nL 183.691682 104.990988 \nL 184.301629 104.54878 \nL 186.131471 106.601326 \nL 186.741418 105.789281 \nL 187.351365 105.354109 \nL 188.57126 105.226938 \nL 189.791154 105.825434 \nL 191.011048 103.547553 \nL 191.620995 103.49418 \nL 192.84089 104.09494 \nL 193.450837 104.039733 \nL 195.280678 104.916374 \nL 195.890626 103.824041 \nL 196.500573 103.771374 \nL 197.11052 104.060775 \nL 197.720467 104.007501 \nL 198.330414 104.631678 \nL 198.940361 104.239342 \nL 200.160256 104.133103 \nL 200.770203 103.748289 \nL 201.38015 103.035977 \nL 201.990097 103.318374 \nL 202.600044 102.942264 \nL 203.209992 102.896026 \nL 204.429886 103.452161 \nL 206.259727 103.309847 \nL 206.869675 102.626939 \nL 208.089569 103.171166 \nL 208.699516 103.125724 \nL 209.91941 103.658895 \nL 210.529358 104.232295 \nL 211.139305 104.492048 \nL 211.749252 105.05723 \nL 212.359199 105.311427 \nL 212.969146 105.258485 \nL 213.579093 105.509777 \nL 214.189041 104.851336 \nL 214.798988 104.800912 \nL 215.408935 105.351115 \nL 216.018882 105.597978 \nL 216.628829 106.14056 \nL 217.238776 106.085784 \nL 217.848724 106.622103 \nL 218.458671 106.565846 \nL 219.068618 106.803033 \nL 219.678565 106.746512 \nL 220.288512 106.981125 \nL 220.898459 106.92435 \nL 221.508407 107.444884 \nL 222.118354 107.674063 \nL 223.948195 106.647 \nL 224.558142 106.875726 \nL 225.778037 106.766405 \nL 226.387984 106.432645 \nL 226.997931 106.658742 \nL 227.607878 107.160775 \nL 228.217825 107.105866 \nL 229.43772 107.546373 \nL 230.047667 107.217119 \nL 230.657614 107.707863 \nL 233.70735 107.432391 \nL 234.927244 108.387723 \nL 235.537191 108.330781 \nL 236.757086 108.743851 \nL 237.367033 108.162399 \nL 237.97698 108.629085 \nL 239.806822 108.459929 \nL 241.026716 108.862097 \nL 241.636663 109.316709 \nL 242.24661 109.513469 \nL 243.466505 109.396968 \nL 244.076452 109.591452 \nL 244.686399 110.035892 \nL 245.296346 109.976465 \nL 246.51624 110.853813 \nL 247.126188 110.792244 \nL 247.736135 110.48401 \nL 248.956029 110.855459 \nL 249.565976 111.283993 \nL 250.175923 110.490505 \nL 250.785871 110.674385 \nL 251.395818 110.372569 \nL 252.005765 110.555642 \nL 252.615712 110.978189 \nL 253.225659 111.158104 \nL 253.835606 111.097749 \nL 254.445554 110.799424 \nL 255.665448 111.629462 \nL 256.885342 111.507671 \nL 257.495289 111.212769 \nL 258.105237 111.621244 \nL 258.715184 111.560945 \nL 259.325131 111.965774 \nL 259.935078 111.904779 \nL 260.545025 112.075084 \nL 261.154972 112.014122 \nL 262.374867 112.350909 \nL 263.594761 112.228797 \nL 264.204708 112.394993 \nL 264.814655 112.334175 \nL 266.03455 111.764387 \nL 267.254444 111.648008 \nL 267.864391 110.92254 \nL 268.474338 110.867223 \nL 269.694233 111.198807 \nL 270.30418 111.583081 \nL 270.914127 111.745775 \nL 272.134021 111.196212 \nL 272.743969 111.358607 \nL 273.353916 111.08656 \nL 273.963863 111.032214 \nL 274.57381 111.193671 \nL 276.403652 111.031596 \nL 277.013599 110.765182 \nL 277.623546 110.925092 \nL 278.233493 111.295878 \nL 278.84344 111.242173 \nL 279.453387 111.399366 \nL 280.063335 111.765657 \nL 280.673282 111.292288 \nL 281.283229 111.030392 \nL 281.893176 110.56174 \nL 282.503123 109.888077 \nL 283.723018 109.790907 \nL 284.942912 110.516176 \nL 286.162806 109.599792 \nL 286.772753 109.552659 \nL 287.382701 109.70888 \nL 288.602595 110.422739 \nL 289.212542 110.575326 \nL 290.432436 110.07664 \nL 291.042384 110.029121 \nL 291.652331 109.782589 \nL 292.262278 109.338646 \nL 292.872225 109.491675 \nL 293.482172 109.841574 \nL 294.092119 109.598157 \nL 294.702067 109.945913 \nL 295.312014 109.703512 \nL 295.921961 109.657981 \nL 296.531908 109.417613 \nL 297.141855 109.567626 \nL 298.36175 109.478222 \nL 299.581644 110.159903 \nL 300.191591 109.921965 \nL 300.801538 110.259901 \nL 302.021433 110.549434 \nL 302.63138 110.883151 \nL 303.241327 110.835996 \nL 303.851274 110.410887 \nL 306.291063 110.978189 \nL 307.510957 110.885062 \nL 308.730851 110.422231 \nL 309.340799 110.747122 \nL 309.950746 110.88599 \nL 310.560693 110.656292 \nL 311.17064 110.611225 \nL 311.780587 110.932426 \nL 312.390534 110.886895 \nL 313.000482 111.205855 \nL 314.830323 111.610982 \nL 315.44027 111.564347 \nL 316.050217 111.697869 \nL 317.270112 111.604863 \nL 317.880059 111.737324 \nL 318.490006 111.512763 \nL 319.7099 112.130887 \nL 320.319848 112.083909 \nL 320.929795 112.390133 \nL 321.539742 112.342757 \nL 322.149689 112.646926 \nL 322.759636 112.599175 \nL 323.369583 112.376811 \nL 323.979531 112.504335 \nL 324.589478 112.457247 \nL 326.419319 112.835688 \nL 327.029266 113.133053 \nL 327.639214 112.74107 \nL 328.859108 112.647335 \nL 329.469055 112.771589 \nL 330.079002 112.554461 \nL 331.298897 112.462428 \nL 331.908844 112.755208 \nL 333.128738 112.325956 \nL 333.738685 112.617058 \nL 334.95858 112.860652 \nL 335.568527 112.480764 \nL 336.178474 112.768905 \nL 336.788421 112.723341 \nL 337.398368 112.512145 \nL 338.008315 112.632809 \nL 338.618263 112.918046 \nL 339.22821 112.872548 \nL 341.667998 113.345685 \nL 342.277946 113.136841 \nL 342.887893 113.254027 \nL 343.49784 113.208496 \nL 344.107787 113.325007 \nL 344.717734 113.27952 \nL 345.327681 113.39538 \nL 345.937629 113.189143 \nL 346.547576 113.144242 \nL 347.157523 113.419747 \nL 347.76747 113.534303 \nL 348.377417 113.488949 \nL 349.597312 113.081369 \nL 350.817206 112.993378 \nL 351.427153 112.791936 \nL 353.866942 113.871345 \nL 354.476889 113.670178 \nL 355.086836 113.937045 \nL 356.306731 113.847132 \nL 357.526625 114.066822 \nL 358.136572 113.867766 \nL 358.746519 113.823317 \nL 359.356466 113.932538 \nL 361.186308 113.7999 \nL 361.796255 113.603848 \nL 362.406202 113.864286 \nL 363.626097 113.77687 \nL 364.236044 113.582452 \nL 364.845991 113.840825 \nL 365.455938 113.947793 \nL 366.065885 113.904261 \nL 366.675832 114.010666 \nL 367.28578 114.266089 \nL 367.895727 114.222182 \nL 368.505674 114.029609 \nL 369.725568 114.536292 \nL 369.725568 114.536292 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_16\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 92.446367 \nL 66.581826 92.446367 \nL 67.191773 129.51 \nL 67.80172 136.922731 \nL 68.411668 141.864544 \nL 69.021615 145.394414 \nL 69.631562 148.041822 \nL 70.241509 141.864544 \nL 71.461403 146.357113 \nL 72.071351 148.041822 \nL 72.681298 143.765243 \nL 73.901192 146.806369 \nL 74.511139 138.775911 \nL 75.731034 150.100913 \nL 76.340981 150.967899 \nL 76.950928 148.041822 \nL 77.560875 152.454159 \nL 78.780769 140.790238 \nL 80.000664 136.922731 \nL 80.610611 138.063146 \nL 81.220558 130.882731 \nL 81.830505 129.51 \nL 82.440452 130.788057 \nL 83.0504 129.51 \nL 83.660347 130.705611 \nL 84.880241 123.894303 \nL 85.490188 122.969359 \nL 86.100135 124.215195 \nL 87.32003 122.497967 \nL 88.539924 124.758252 \nL 89.149871 122.09728 \nL 90.369766 124.215195 \nL 90.979713 121.752496 \nL 91.58966 121.086455 \nL 92.199607 118.802731 \nL 92.809554 119.841226 \nL 94.029449 109.433873 \nL 94.639396 110.599991 \nL 95.249343 107.271818 \nL 95.85929 108.434601 \nL 96.469237 108.127141 \nL 97.079185 106.432645 \nL 97.689132 102.055461 \nL 98.909026 101.712278 \nL 99.518973 98.948757 \nL 100.12892 98.836653 \nL 100.738868 99.984733 \nL 101.348815 99.859098 \nL 101.958762 100.952778 \nL 102.568709 103.206778 \nL 103.788603 102.870521 \nL 104.398551 103.850562 \nL 105.008498 105.924051 \nL 106.228392 107.707863 \nL 106.838339 107.486682 \nL 107.448286 108.330781 \nL 108.058234 107.063019 \nL 108.668181 107.889556 \nL 109.278128 106.662564 \nL 109.888075 108.473891 \nL 110.498022 108.260187 \nL 111.107969 109.027475 \nL 111.717917 108.812125 \nL 112.327864 107.65196 \nL 112.937811 107.459487 \nL 113.547758 108.198419 \nL 115.3776 107.629062 \nL 115.987547 106.565846 \nL 116.597494 107.271818 \nL 117.207441 106.237487 \nL 117.817388 104.374896 \nL 118.427335 105.081696 \nL 119.037283 104.939724 \nL 119.64723 106.448186 \nL 120.257177 107.108903 \nL 120.867124 108.560998 \nL 121.477071 108.387723 \nL 122.087018 107.429542 \nL 122.696966 108.052106 \nL 123.306913 109.433873 \nL 123.91686 109.258741 \nL 124.526807 108.330781 \nL 125.136754 108.919093 \nL 125.746701 110.236915 \nL 126.356649 110.060767 \nL 128.18649 111.690954 \nL 128.796437 109.389744 \nL 130.016332 111.84416 \nL 130.626279 111.664555 \nL 131.236226 109.448033 \nL 131.846173 109.967364 \nL 132.45612 109.809519 \nL 133.066067 108.992636 \nL 133.676015 108.846213 \nL 134.285962 110.002832 \nL 135.505856 110.978189 \nL 136.115803 109.552659 \nL 136.72575 110.035892 \nL 137.945645 109.742736 \nL 138.555592 110.212404 \nL 139.775486 109.923534 \nL 140.385433 110.380389 \nL 141.605328 112.448963 \nL 142.215275 112.29146 \nL 142.825222 110.978189 \nL 143.435169 110.834527 \nL 144.045116 111.263293 \nL 144.655064 112.251364 \nL 145.265011 112.101329 \nL 146.484905 112.914345 \nL 147.094852 112.76273 \nL 147.704799 113.703459 \nL 148.924694 112.321074 \nL 149.534641 113.244671 \nL 150.754535 114.001111 \nL 151.364482 113.849308 \nL 151.97443 114.218018 \nL 153.194324 115.962606 \nL 153.804271 115.293815 \nL 154.414218 115.138389 \nL 155.024165 115.485924 \nL 155.634113 114.336296 \nL 156.854007 114.046366 \nL 157.463954 113.416588 \nL 158.683848 113.144242 \nL 159.293796 113.488949 \nL 159.903743 113.354058 \nL 160.51369 112.748746 \nL 161.123637 113.558571 \nL 161.733584 112.493367 \nL 162.343531 111.904779 \nL 162.953479 112.704751 \nL 163.563426 113.037274 \nL 164.173373 112.456187 \nL 164.78332 112.78617 \nL 165.393267 112.213643 \nL 166.613162 111.976908 \nL 167.223109 112.743124 \nL 167.833056 113.061641 \nL 168.443003 113.81247 \nL 169.05295 113.68752 \nL 170.882792 114.599342 \nL 172.102686 116.032316 \nL 173.32258 114.934417 \nL 173.932528 115.637019 \nL 174.542475 114.684549 \nL 175.152422 114.971232 \nL 175.762369 114.847464 \nL 176.372316 115.535188 \nL 176.982263 115.006844 \nL 177.592211 113.682825 \nL 178.202158 113.967189 \nL 178.812105 114.644906 \nL 179.422052 114.526837 \nL 180.031999 114.802209 \nL 180.641946 114.684549 \nL 181.251894 115.344327 \nL 181.861841 114.838981 \nL 182.471788 115.491115 \nL 184.301629 116.272994 \nL 184.911577 116.152042 \nL 185.521524 116.78108 \nL 186.131471 116.658791 \nL 186.741418 117.279003 \nL 187.351365 117.155456 \nL 187.961312 116.299206 \nL 188.57126 116.546864 \nL 189.181207 116.065354 \nL 189.791154 116.673328 \nL 191.011048 117.155456 \nL 192.84089 116.802475 \nL 193.450837 117.038359 \nL 194.060784 116.223045 \nL 195.280678 116.000825 \nL 195.890626 116.236057 \nL 196.500573 116.125919 \nL 197.720467 115.2286 \nL 198.330414 114.109128 \nL 198.940361 113.673724 \nL 199.550309 113.913087 \nL 200.160256 113.816392 \nL 200.770203 113.388156 \nL 201.990097 114.519823 \nL 202.600044 114.422156 \nL 203.819939 114.879618 \nL 205.039833 114.684549 \nL 205.64978 114.909177 \nL 206.259727 115.451383 \nL 206.869675 115.670798 \nL 207.479622 115.57154 \nL 208.089569 116.104004 \nL 210.529358 116.948689 \nL 211.139305 116.846592 \nL 211.749252 117.052928 \nL 212.359199 117.563875 \nL 212.969146 116.850414 \nL 213.579093 116.44659 \nL 214.189041 116.651192 \nL 214.798988 116.251466 \nL 216.628829 115.964661 \nL 217.238776 116.167098 \nL 218.458671 115.390521 \nL 219.068618 115.592825 \nL 220.898459 115.321584 \nL 221.508407 114.655708 \nL 222.118354 114.282304 \nL 222.728301 114.198003 \nL 223.338248 114.684549 \nL 223.948195 114.883363 \nL 224.558142 114.231869 \nL 225.778037 113.505253 \nL 226.387984 113.705514 \nL 226.997931 113.346911 \nL 228.827773 113.940525 \nL 229.43772 113.311818 \nL 230.047667 113.234829 \nL 231.267561 114.168643 \nL 231.877508 114.089367 \nL 232.487456 114.280217 \nL 233.097403 114.738265 \nL 233.70735 114.657785 \nL 234.317297 114.844537 \nL 234.927244 114.498571 \nL 235.537191 114.419814 \nL 236.147139 114.605406 \nL 237.367033 114.448809 \nL 237.97698 114.632347 \nL 238.586927 114.554507 \nL 239.196874 114.995577 \nL 239.806822 115.175293 \nL 240.416769 114.838981 \nL 241.026716 114.761494 \nL 241.636663 114.428938 \nL 242.24661 114.353395 \nL 243.466505 114.709855 \nL 244.076452 115.138389 \nL 244.686399 114.558914 \nL 245.296346 114.734642 \nL 246.51624 114.585048 \nL 247.126188 114.263096 \nL 247.736135 114.437454 \nL 248.346082 114.856941 \nL 248.956029 114.782735 \nL 249.565976 114.21973 \nL 250.175923 114.148098 \nL 251.395818 114.49075 \nL 252.005765 114.418952 \nL 252.615712 114.828951 \nL 253.225659 114.756524 \nL 254.445554 114.136345 \nL 255.055501 113.591653 \nL 255.665448 113.287263 \nL 256.275395 113.45696 \nL 256.885342 113.390266 \nL 257.495289 113.089411 \nL 258.105237 113.491965 \nL 258.715184 113.192678 \nL 259.935078 113.063011 \nL 260.545025 112.536943 \nL 261.154972 112.474545 \nL 262.984814 112.973927 \nL 263.594761 112.910954 \nL 264.814655 112.334175 \nL 265.424603 112.273721 \nL 266.03455 111.989015 \nL 266.644497 111.482078 \nL 267.254444 111.871288 \nL 267.864391 112.035562 \nL 268.474338 112.420785 \nL 269.084286 112.139878 \nL 269.694233 112.301888 \nL 270.30418 112.023003 \nL 270.914127 111.965089 \nL 271.524074 111.470181 \nL 272.134021 111.850268 \nL 272.743969 111.575989 \nL 273.353916 111.953546 \nL 273.963863 112.112783 \nL 274.57381 112.486596 \nL 275.183757 112.428506 \nL 275.793704 112.584992 \nL 276.403652 112.954199 \nL 278.233493 113.413793 \nL 279.453387 112.873487 \nL 280.063335 113.02561 \nL 280.673282 112.967476 \nL 281.283229 113.118483 \nL 282.503123 113.833225 \nL 283.11307 113.980544 \nL 284.332965 113.860917 \nL 284.942912 114.006932 \nL 285.552859 113.947373 \nL 286.162806 114.296554 \nL 286.772753 114.440171 \nL 287.382701 114.379916 \nL 287.992648 114.522529 \nL 288.602595 114.462373 \nL 289.212542 114.805413 \nL 289.822489 114.744815 \nL 291.042384 113.825394 \nL 292.262278 114.505695 \nL 292.872225 114.644906 \nL 293.482172 114.585711 \nL 294.092119 114.329691 \nL 295.312014 114.213898 \nL 295.921961 114.547636 \nL 296.531908 114.294411 \nL 297.141855 114.431622 \nL 298.36175 113.929733 \nL 298.971697 113.873786 \nL 299.581644 113.625586 \nL 300.191591 113.762764 \nL 300.801538 114.09077 \nL 302.021433 114.360597 \nL 302.63138 114.304407 \nL 303.851274 114.571086 \nL 304.461221 114.514797 \nL 305.681116 115.153709 \nL 306.291063 115.283564 \nL 306.90101 115.599475 \nL 307.510957 115.541296 \nL 308.730851 115.796455 \nL 309.340799 115.368518 \nL 309.950746 115.311499 \nL 310.560693 115.622638 \nL 311.17064 115.748759 \nL 311.780587 115.691221 \nL 313.000482 115.212727 \nL 313.610429 115.338617 \nL 314.220376 115.645127 \nL 314.830323 115.407741 \nL 316.660165 115.779411 \nL 317.270112 115.543992 \nL 318.490006 115.432948 \nL 319.099953 115.200069 \nL 319.7099 114.790954 \nL 320.319848 114.914534 \nL 321.539742 114.807798 \nL 322.149689 114.403499 \nL 322.759636 114.526837 \nL 323.369583 114.824411 \nL 324.589478 114.719355 \nL 325.809372 114.961667 \nL 326.419319 114.736387 \nL 327.029266 114.684549 \nL 327.639214 114.46096 \nL 328.859108 114.70167 \nL 329.469055 114.650395 \nL 330.079002 114.940161 \nL 330.688949 114.888566 \nL 331.298897 114.667583 \nL 331.908844 114.616849 \nL 333.128738 114.85302 \nL 333.738685 114.802209 \nL 334.348632 114.919339 \nL 334.95858 114.868617 \nL 335.568527 115.152019 \nL 336.178474 114.767835 \nL 336.788421 114.883993 \nL 337.398368 114.502138 \nL 338.008315 114.452907 \nL 338.618263 114.734079 \nL 339.22821 114.355096 \nL 339.838157 114.635241 \nL 340.448104 114.258158 \nL 341.058051 114.210009 \nL 342.887893 114.554507 \nL 343.49784 114.830531 \nL 344.107787 114.781664 \nL 344.717734 114.571506 \nL 345.327681 114.684549 \nL 345.937629 114.9579 \nL 347.157523 114.860664 \nL 347.76747 114.972116 \nL 348.377417 114.923669 \nL 348.987365 114.716361 \nL 349.597312 114.827405 \nL 350.207259 115.096371 \nL 350.817206 115.048078 \nL 351.427153 114.842272 \nL 352.0371 114.952101 \nL 352.647048 114.590317 \nL 353.256995 114.543505 \nL 354.476889 114.762577 \nL 356.306731 114.622516 \nL 356.916678 114.730975 \nL 357.526625 114.375686 \nL 358.136572 114.484211 \nL 358.746519 114.438481 \nL 360.576361 114.760975 \nL 361.186308 115.02011 \nL 361.796255 114.973751 \nL 364.845991 115.49814 \nL 367.28578 115.312239 \nL 369.115621 115.620429 \nL 369.725568 115.425823 \nL 369.725568 115.425823 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_17\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 240.700909 \nL 65.971879 166.573638 \nL 66.581826 141.864544 \nL 67.191773 148.041822 \nL 67.80172 107.271818 \nL 68.411668 117.155456 \nL 69.021615 113.625586 \nL 69.631562 120.2441 \nL 70.241509 117.155456 \nL 70.851456 122.09728 \nL 71.461403 112.662898 \nL 72.071351 110.978189 \nL 72.681298 103.850562 \nL 73.901192 82.56273 \nL 75.121086 92.446367 \nL 75.731034 100.68273 \nL 76.950928 107.271818 \nL 78.170822 112.662898 \nL 78.780769 115.006844 \nL 79.390717 110.978189 \nL 80.610611 115.254757 \nL 81.220558 119.900917 \nL 81.830505 121.567799 \nL 82.440452 120.563612 \nL 83.0504 124.568187 \nL 83.660347 123.532 \nL 84.270294 127.193525 \nL 84.880241 126.140582 \nL 85.490188 127.329786 \nL 86.100135 130.568963 \nL 86.710083 129.51 \nL 87.32003 130.511724 \nL 87.929977 133.41144 \nL 89.759819 135.837943 \nL 90.369766 133.03987 \nL 90.979713 135.543616 \nL 91.58966 132.879429 \nL 92.199607 135.275456 \nL 94.029449 137.231594 \nL 94.639396 136.317608 \nL 95.85929 140.411068 \nL 96.469237 138.063146 \nL 97.079185 139.999711 \nL 98.299079 135.574964 \nL 98.909026 137.452213 \nL 100.12892 138.456399 \nL 100.738868 137.676564 \nL 101.348815 139.393638 \nL 101.958762 138.624009 \nL 102.568709 136.683611 \nL 103.178656 137.158051 \nL 103.788603 136.459436 \nL 104.398551 134.64189 \nL 105.008498 136.248847 \nL 105.618445 134.488706 \nL 106.228392 136.050641 \nL 107.448286 134.804805 \nL 108.058234 136.2963 \nL 109.278128 137.125819 \nL 109.888075 135.52032 \nL 110.498022 134.946003 \nL 111.107969 133.41144 \nL 111.717917 132.879429 \nL 112.327864 133.311398 \nL 112.937811 131.855802 \nL 113.547758 133.216371 \nL 114.157705 130.882731 \nL 115.987547 132.157408 \nL 116.597494 133.434393 \nL 117.207441 133.819727 \nL 117.817388 132.492141 \nL 118.427335 132.037075 \nL 119.037283 132.425125 \nL 119.64723 133.628181 \nL 120.867124 134.344393 \nL 121.477071 133.893878 \nL 122.087018 134.241534 \nL 122.696966 133.021291 \nL 123.91686 132.184702 \nL 124.526807 131.022803 \nL 125.136754 131.381903 \nL 125.746701 130.251274 \nL 126.356649 131.344833 \nL 126.966596 130.236738 \nL 128.18649 130.93553 \nL 128.796437 131.980918 \nL 130.626279 130.882731 \nL 131.846173 131.531662 \nL 132.45612 129.843915 \nL 133.066067 130.833709 \nL 134.895909 131.766054 \nL 135.505856 131.427091 \nL 136.115803 132.361049 \nL 136.72575 130.138198 \nL 137.335698 131.067295 \nL 137.945645 131.363191 \nL 138.555592 131.041559 \nL 139.165539 131.332804 \nL 140.385433 129.51 \nL 140.995381 129.806514 \nL 141.605328 130.686623 \nL 142.215275 130.969198 \nL 142.825222 130.668243 \nL 144.655064 131.490505 \nL 145.265011 131.19472 \nL 145.874958 130.346025 \nL 146.484905 130.616383 \nL 147.094852 130.333643 \nL 149.534641 133.509682 \nL 150.754535 132.927224 \nL 151.364482 133.686194 \nL 151.97443 132.879429 \nL 152.584377 132.598644 \nL 153.194324 132.832948 \nL 153.804271 131.540885 \nL 154.414218 132.283474 \nL 155.024165 130.511724 \nL 155.634113 130.256256 \nL 156.24406 129.51 \nL 156.854007 129.75546 \nL 157.463954 130.485368 \nL 158.073901 130.72123 \nL 158.683848 131.435387 \nL 159.293796 130.705611 \nL 159.903743 130.93553 \nL 160.51369 129.746082 \nL 161.123637 130.448321 \nL 162.343531 129.973295 \nL 163.563426 130.425158 \nL 164.173373 129.282622 \nL 164.78332 128.60601 \nL 166.613162 127.956438 \nL 167.223109 127.30384 \nL 167.833056 127.974829 \nL 168.443003 128.201876 \nL 169.05295 127.992779 \nL 169.662897 128.648061 \nL 170.882792 129.083985 \nL 171.492739 127.180292 \nL 172.102686 126.982936 \nL 173.932528 127.64647 \nL 174.542475 128.274547 \nL 175.762369 128.695415 \nL 176.372316 128.497341 \nL 177.592211 128.908975 \nL 178.202158 128.3144 \nL 178.812105 126.536983 \nL 179.422052 127.144239 \nL 180.641946 127.559285 \nL 181.251894 126.987344 \nL 183.081735 127.599503 \nL 185.521524 126.889345 \nL 186.131471 127.088766 \nL 187.351365 128.219229 \nL 187.961312 128.042143 \nL 188.57126 127.501626 \nL 189.181207 128.056524 \nL 189.791154 127.882818 \nL 190.401101 128.070641 \nL 191.620995 127.728099 \nL 192.84089 128.098057 \nL 193.450837 126.875151 \nL 194.060784 127.412067 \nL 194.670731 127.247904 \nL 195.280678 127.778059 \nL 195.890626 127.958504 \nL 196.500573 127.450915 \nL 197.11052 127.972808 \nL 198.330414 127.648359 \nL 198.940361 128.162233 \nL 199.550309 127.329786 \nL 200.160256 127.506564 \nL 200.770203 128.014163 \nL 201.38015 127.524458 \nL 201.990097 127.368546 \nL 202.600044 127.870015 \nL 203.209992 128.040519 \nL 203.819939 128.534643 \nL 204.429886 128.70075 \nL 205.039833 128.543126 \nL 205.64978 128.707764 \nL 206.259727 128.551465 \nL 206.869675 129.032787 \nL 207.479622 128.55965 \nL 208.089569 128.721417 \nL 208.699516 128.567714 \nL 209.309463 128.10253 \nL 209.91941 127.329786 \nL 210.529358 127.804149 \nL 211.749252 126.895553 \nL 212.359199 126.753205 \nL 212.969146 126.91707 \nL 213.579093 126.472001 \nL 214.798988 127.400689 \nL 215.408935 126.959066 \nL 216.018882 126.8199 \nL 220.288512 127.911183 \nL 221.508407 128.788918 \nL 223.338248 128.369585 \nL 223.948195 128.799974 \nL 225.778037 129.229215 \nL 226.387984 129.649862 \nL 226.997931 129.51 \nL 227.607878 128.815925 \nL 228.217825 128.956814 \nL 228.827773 128.545523 \nL 229.43772 128.686368 \nL 230.047667 129.099703 \nL 231.267561 129.374236 \nL 231.877508 129.239466 \nL 232.487456 129.37523 \nL 233.097403 129.778579 \nL 233.70735 129.911416 \nL 234.317297 129.51 \nL 234.927244 129.642848 \nL 235.537191 129.51 \nL 236.147139 129.114305 \nL 236.757086 128.984274 \nL 237.367033 129.1171 \nL 237.97698 128.46596 \nL 238.586927 128.339573 \nL 239.196874 128.473261 \nL 239.806822 128.089452 \nL 241.026716 128.868768 \nL 241.636663 128.743166 \nL 242.24661 129.127903 \nL 242.856557 129.002279 \nL 243.466505 129.13051 \nL 244.076452 127.997208 \nL 244.686399 128.127969 \nL 245.906293 127.887689 \nL 246.51624 128.266262 \nL 247.126188 128.394372 \nL 247.736135 128.768737 \nL 248.956029 129.01909 \nL 249.565976 128.898393 \nL 250.175923 129.022327 \nL 250.785871 128.902402 \nL 251.395818 129.267754 \nL 252.005765 129.38928 \nL 253.225659 129.15016 \nL 253.835606 128.79264 \nL 256.275395 129.273929 \nL 256.885342 128.921688 \nL 257.495289 129.04084 \nL 258.715184 127.878267 \nL 259.325131 127.767201 \nL 259.935078 127.888473 \nL 260.545025 127.778059 \nL 261.154972 127.438124 \nL 261.76492 127.329786 \nL 262.374867 127.450915 \nL 262.984814 127.343207 \nL 263.594761 127.008773 \nL 264.204708 126.903086 \nL 264.814655 127.250024 \nL 265.424603 127.369551 \nL 266.03455 127.263721 \nL 266.644497 127.606429 \nL 267.254444 127.500533 \nL 267.864391 127.617861 \nL 268.474338 127.512562 \nL 269.084286 127.850442 \nL 269.694233 127.745065 \nL 270.30418 127.420362 \nL 270.914127 126.878266 \nL 271.524074 127.214026 \nL 272.134021 126.67573 \nL 272.743969 127.010109 \nL 273.963863 126.808578 \nL 274.57381 126.277716 \nL 275.183757 126.394514 \nL 275.793704 126.724873 \nL 276.403652 126.412464 \nL 277.013599 126.314863 \nL 277.623546 126.430214 \nL 278.233493 126.121329 \nL 278.84344 126.025385 \nL 279.453387 125.719405 \nL 280.063335 125.205166 \nL 280.673282 124.903229 \nL 282.503123 125.253392 \nL 283.11307 124.954702 \nL 283.723018 124.864149 \nL 284.332965 125.185913 \nL 284.942912 125.095227 \nL 285.552859 125.414573 \nL 286.162806 124.915335 \nL 286.772753 124.622488 \nL 287.382701 124.737431 \nL 287.992648 124.649197 \nL 289.212542 124.071323 \nL 291.042384 125.014416 \nL 291.652331 124.7276 \nL 294.092119 125.172769 \nL 295.921961 124.913722 \nL 296.531908 125.023352 \nL 297.141855 124.548735 \nL 297.751802 124.270645 \nL 298.36175 123.800458 \nL 298.971697 123.718807 \nL 299.581644 123.830127 \nL 300.801538 124.434101 \nL 301.411485 124.542715 \nL 302.021433 124.26964 \nL 302.63138 123.807903 \nL 303.241327 123.727699 \nL 303.851274 123.836997 \nL 304.461221 123.757125 \nL 305.071168 123.489518 \nL 305.681116 123.598584 \nL 306.291063 123.14554 \nL 306.90101 122.8815 \nL 307.510957 123.177529 \nL 308.120904 123.286286 \nL 309.950746 123.056135 \nL 310.560693 123.348065 \nL 311.17064 123.088091 \nL 313.000482 122.862225 \nL 313.610429 122.242621 \nL 314.220376 121.807294 \nL 314.830323 122.09728 \nL 316.050217 122.313182 \nL 316.660165 121.881898 \nL 317.270112 121.810796 \nL 317.880059 121.382803 \nL 318.490006 121.491428 \nL 319.099953 121.244012 \nL 319.7099 121.35246 \nL 320.319848 120.929637 \nL 320.929795 121.214807 \nL 321.539742 121.146478 \nL 322.149689 120.727156 \nL 322.759636 120.835538 \nL 323.369583 120.768578 \nL 323.979531 120.527525 \nL 325.199425 120.743195 \nL 325.809372 120.503887 \nL 327.639214 120.824558 \nL 328.249161 120.758869 \nL 328.859108 120.522289 \nL 330.688949 120.329109 \nL 331.908844 120.540272 \nL 332.518791 120.476272 \nL 333.128738 120.581042 \nL 333.738685 120.853421 \nL 334.348632 120.789145 \nL 334.95858 120.892501 \nL 335.568527 121.162339 \nL 336.178474 121.09781 \nL 337.398368 121.632947 \nL 338.008315 121.402332 \nL 338.618263 120.677472 \nL 339.22821 120.614732 \nL 339.838157 120.71664 \nL 340.448104 120.326093 \nL 342.277946 120.142269 \nL 343.49784 119.696658 \nL 344.107787 119.799009 \nL 344.717734 120.062407 \nL 345.327681 120.002373 \nL 345.937629 120.264192 \nL 346.547576 119.883089 \nL 348.987365 120.283865 \nL 349.597312 120.541719 \nL 350.817206 120.421905 \nL 352.0371 120.61788 \nL 353.256995 120.498762 \nL 353.866942 120.126805 \nL 356.306731 119.895174 \nL 356.916678 120.147372 \nL 357.526625 119.780805 \nL 359.356466 120.071443 \nL 359.966414 119.554896 \nL 360.576361 119.193322 \nL 361.186308 119.443333 \nL 361.796255 119.387905 \nL 363.016149 118.974537 \nL 363.626097 119.222958 \nL 364.845991 119.114112 \nL 365.455938 118.909654 \nL 366.065885 119.156197 \nL 366.675832 118.802731 \nL 367.28578 118.600139 \nL 367.895727 118.547517 \nL 368.505674 118.792811 \nL 369.115621 118.888557 \nL 369.725568 118.83568 \nL 369.725568 118.83568 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_18\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 166.573638 \nL 66.581826 92.446367 \nL 67.191773 73.914557 \nL 67.80172 92.446367 \nL 68.411668 117.155456 \nL 69.021615 134.804805 \nL 69.631562 148.041822 \nL 70.241509 141.864544 \nL 70.851456 129.51 \nL 71.461403 126.140582 \nL 72.071351 135.687278 \nL 72.681298 132.361049 \nL 73.291245 124.215195 \nL 73.901192 127.039093 \nL 74.511139 134.142961 \nL 75.731034 129.51 \nL 76.950928 118.390909 \nL 77.560875 124.215195 \nL 78.170822 122.771164 \nL 78.780769 127.898547 \nL 79.390717 123.332733 \nL 80.610611 109.552659 \nL 81.220558 114.410005 \nL 83.0504 112.213643 \nL 83.660347 113.967189 \nL 84.270294 113.294664 \nL 84.880241 117.155456 \nL 85.490188 112.068291 \nL 86.100135 111.507671 \nL 86.710083 113.037274 \nL 87.32003 112.480764 \nL 88.539924 115.254757 \nL 89.149871 112.831369 \nL 89.759819 112.334175 \nL 90.369766 106.565846 \nL 90.979713 106.237487 \nL 91.58966 107.608771 \nL 92.199607 105.624554 \nL 92.809554 106.949534 \nL 94.639396 106.061582 \nL 95.249343 104.306732 \nL 97.079185 107.831278 \nL 97.689132 110.291824 \nL 98.909026 112.301888 \nL 99.518973 110.653066 \nL 100.12892 110.339155 \nL 100.738868 111.292288 \nL 101.958762 110.674385 \nL 104.398551 114.114342 \nL 105.008498 111.539759 \nL 106.228392 113.158403 \nL 106.838339 115.006844 \nL 107.448286 115.743512 \nL 108.058234 114.371334 \nL 108.668181 114.066822 \nL 109.278128 114.786093 \nL 109.888075 114.484211 \nL 110.498022 113.202001 \nL 111.107969 113.904261 \nL 111.717917 113.625586 \nL 112.327864 114.304407 \nL 112.937811 115.904362 \nL 113.547758 116.537729 \nL 114.157705 116.240309 \nL 114.767652 116.854126 \nL 115.3776 116.560064 \nL 115.987547 118.037929 \nL 117.207441 119.166668 \nL 117.817388 120.563612 \nL 118.427335 121.086455 \nL 119.64723 118.802731 \nL 122.087018 120.835538 \nL 122.696966 118.976128 \nL 123.306913 118.699772 \nL 124.526807 121.189601 \nL 125.136754 120.150498 \nL 125.746701 121.356006 \nL 126.356649 121.803704 \nL 126.966596 120.789145 \nL 127.576543 118.354933 \nL 128.18649 116.680287 \nL 128.796437 117.155456 \nL 129.406384 116.922356 \nL 130.016332 114.61527 \nL 131.236226 114.208507 \nL 131.846173 115.358433 \nL 133.066067 114.949295 \nL 133.676015 114.094161 \nL 134.895909 115.006844 \nL 135.505856 114.81236 \nL 136.72575 115.689664 \nL 137.335698 115.494341 \nL 137.945645 115.920002 \nL 141.605328 114.802209 \nL 142.215275 114.0425 \nL 142.825222 112.715543 \nL 144.045116 111.263293 \nL 144.655064 111.119653 \nL 145.874958 111.953546 \nL 146.484905 111.807974 \nL 147.094852 112.213643 \nL 147.704799 111.523245 \nL 148.924694 112.321074 \nL 149.534641 112.178086 \nL 150.144588 113.096104 \nL 150.754535 113.475385 \nL 151.364482 112.283242 \nL 152.584377 113.037274 \nL 153.194324 112.384046 \nL 153.804271 112.247487 \nL 154.414218 111.608519 \nL 155.024165 111.479051 \nL 155.634113 111.84881 \nL 156.24406 112.707822 \nL 156.854007 113.064547 \nL 157.463954 112.928904 \nL 158.073901 113.27952 \nL 158.683848 113.144242 \nL 159.293796 113.488949 \nL 159.903743 113.354058 \nL 160.51369 113.693043 \nL 161.123637 114.496892 \nL 161.733584 114.3582 \nL 162.343531 115.147844 \nL 162.953479 114.086009 \nL 163.563426 113.494859 \nL 164.173373 114.275257 \nL 164.78332 114.142155 \nL 165.393267 114.459921 \nL 166.003214 114.327305 \nL 166.613162 114.640167 \nL 167.223109 115.390521 \nL 167.833056 114.377519 \nL 168.443003 114.248504 \nL 169.05295 114.554507 \nL 170.272845 114.298917 \nL 170.882792 113.321296 \nL 171.492739 114.049171 \nL 172.712633 114.642674 \nL 173.932528 113.566425 \nL 175.152422 113.333059 \nL 175.762369 113.625586 \nL 176.372316 113.104786 \nL 177.592211 113.682825 \nL 178.202158 113.170122 \nL 178.812105 113.852103 \nL 179.422052 114.132534 \nL 180.031999 114.802209 \nL 180.641946 113.904261 \nL 181.861841 114.452907 \nL 183.081735 114.226026 \nL 183.691682 114.874626 \nL 184.301629 114.381993 \nL 184.911577 114.27064 \nL 186.131471 113.306306 \nL 188.57126 115.816547 \nL 189.181207 116.065354 \nL 189.791154 115.588539 \nL 191.011048 116.797361 \nL 192.230943 116.564327 \nL 192.84089 115.743512 \nL 193.450837 116.335723 \nL 194.060784 115.873389 \nL 194.670731 115.763395 \nL 195.280678 114.961667 \nL 198.330414 116.140013 \nL 198.940361 116.032316 \nL 199.550309 116.261009 \nL 200.160256 116.821552 \nL 201.38015 117.26577 \nL 201.990097 117.155456 \nL 202.600044 117.374119 \nL 203.209992 117.264312 \nL 203.819939 116.505221 \nL 204.429886 116.723862 \nL 205.039833 116.618308 \nL 205.64978 116.192768 \nL 206.869675 114.716361 \nL 207.479622 114.62119 \nL 208.089569 113.895966 \nL 209.309463 114.340505 \nL 211.139305 114.066822 \nL 211.749252 113.361944 \nL 212.359199 112.96921 \nL 213.579093 114.016188 \nL 214.189041 113.928153 \nL 214.798988 113.539495 \nL 215.408935 113.4541 \nL 216.018882 113.967189 \nL 216.628829 113.285364 \nL 217.238776 112.312481 \nL 217.848724 112.528658 \nL 218.458671 113.037274 \nL 219.068618 113.248879 \nL 219.678565 113.166985 \nL 220.288512 112.795028 \nL 221.508407 113.213544 \nL 223.338248 114.684549 \nL 226.387984 114.264962 \nL 226.997931 114.740286 \nL 227.607878 114.934417 \nL 228.827773 114.767216 \nL 229.43772 114.959093 \nL 230.657614 114.79356 \nL 231.877508 115.712595 \nL 232.487456 115.897546 \nL 234.317297 115.644475 \nL 234.927244 115.827019 \nL 236.147139 115.660602 \nL 236.757086 115.31542 \nL 237.97698 115.676398 \nL 238.586927 115.594891 \nL 239.196874 114.736387 \nL 239.806822 114.658724 \nL 241.026716 113.479019 \nL 241.636663 113.662103 \nL 242.24661 112.824996 \nL 242.856557 112.501353 \nL 244.076452 113.373454 \nL 244.686399 113.302518 \nL 245.296346 112.981626 \nL 246.51624 112.843807 \nL 247.126188 113.271424 \nL 247.736135 113.202001 \nL 248.346082 113.379319 \nL 248.956029 113.310007 \nL 250.785871 114.563034 \nL 251.395818 114.49075 \nL 252.005765 114.901865 \nL 252.615712 114.588285 \nL 253.225659 114.756524 \nL 253.835606 115.162789 \nL 254.445554 114.851396 \nL 255.055501 114.779587 \nL 256.275395 115.109482 \nL 256.885342 115.037541 \nL 258.105237 115.830366 \nL 258.715184 115.989934 \nL 259.325131 115.916136 \nL 260.545025 116.23176 \nL 261.154972 116.157885 \nL 261.76492 115.854976 \nL 262.984814 116.623268 \nL 263.594761 116.776485 \nL 264.204708 116.248704 \nL 264.814655 116.176133 \nL 266.03455 116.481572 \nL 266.644497 116.856865 \nL 267.254444 117.006613 \nL 268.474338 117.747291 \nL 269.084286 117.45049 \nL 270.30418 117.3021 \nL 270.914127 117.447872 \nL 271.524074 117.374119 \nL 273.353916 117.805701 \nL 274.57381 117.227287 \nL 275.183757 117.585183 \nL 276.403652 117.013042 \nL 277.013599 117.155456 \nL 277.623546 117.084663 \nL 278.233493 117.226061 \nL 278.84344 117.155456 \nL 279.453387 117.295848 \nL 280.063335 117.015461 \nL 280.673282 117.364863 \nL 281.283229 117.503477 \nL 281.893176 117.433093 \nL 283.11307 117.707614 \nL 284.942912 117.497689 \nL 286.772753 117.902165 \nL 287.382701 117.426244 \nL 287.992648 117.155456 \nL 288.602595 117.290115 \nL 289.212542 116.81974 \nL 289.822489 116.753686 \nL 290.432436 117.088673 \nL 291.042384 117.222062 \nL 292.262278 117.089214 \nL 292.872225 116.825119 \nL 293.482172 116.364774 \nL 294.092119 116.695452 \nL 294.702067 116.827748 \nL 295.312014 117.155456 \nL 295.921961 116.894675 \nL 296.531908 117.025413 \nL 298.36175 117.994154 \nL 298.971697 118.120662 \nL 299.581644 118.053967 \nL 300.191591 118.179669 \nL 301.411485 117.282825 \nL 302.63138 117.535598 \nL 303.241327 117.471433 \nL 303.851274 117.596692 \nL 304.461221 117.532693 \nL 305.071168 117.657168 \nL 305.681116 117.593345 \nL 307.510957 116.845046 \nL 308.120904 117.155456 \nL 308.730851 117.093688 \nL 309.340799 117.21708 \nL 309.950746 116.971068 \nL 310.560693 116.910205 \nL 311.780587 117.155456 \nL 312.390534 117.094604 \nL 313.000482 117.216175 \nL 313.610429 117.51883 \nL 314.220376 117.638766 \nL 315.44027 117.516179 \nL 316.050217 117.275402 \nL 316.660165 117.394775 \nL 317.880059 117.989018 \nL 318.490006 117.927614 \nL 319.099953 118.222041 \nL 319.7099 117.983042 \nL 322.149689 118.443609 \nL 323.979531 118.260104 \nL 324.589478 118.025502 \nL 325.199425 117.96559 \nL 325.809372 118.079163 \nL 327.029266 117.959945 \nL 327.639214 118.072734 \nL 328.249161 117.841821 \nL 329.469055 118.407997 \nL 330.079002 118.348316 \nL 330.688949 118.628936 \nL 331.298897 118.738654 \nL 331.908844 118.67862 \nL 332.518791 118.787719 \nL 333.128738 119.064793 \nL 334.95858 118.382547 \nL 335.568527 118.491084 \nL 338.618263 118.201053 \nL 340.448104 118.522111 \nL 341.058051 118.137275 \nL 341.667998 118.080687 \nL 342.277946 118.187268 \nL 342.887893 118.130813 \nL 344.107787 118.342363 \nL 345.937629 118.173837 \nL 346.547576 118.439047 \nL 347.157523 118.54301 \nL 347.76747 118.167253 \nL 348.377417 117.952522 \nL 348.987365 118.056861 \nL 349.597312 117.84329 \nL 350.207259 118.105805 \nL 350.817206 118.051095 \nL 351.427153 118.312053 \nL 352.647048 118.516555 \nL 353.256995 118.304719 \nL 353.866942 118.250163 \nL 354.476889 118.351895 \nL 355.086836 118.297472 \nL 355.696783 118.087878 \nL 356.306731 118.189312 \nL 356.916678 118.135574 \nL 357.526625 118.390909 \nL 358.136572 118.491084 \nL 358.746519 118.74463 \nL 359.966414 118.942272 \nL 360.576361 119.193322 \nL 361.186308 118.528187 \nL 361.796255 118.322414 \nL 362.406202 118.269394 \nL 363.016149 118.368177 \nL 363.626097 118.315278 \nL 364.236044 118.564527 \nL 364.845991 118.360776 \nL 365.455938 118.458576 \nL 366.675832 118.353475 \nL 367.28578 118.450689 \nL 367.895727 118.696669 \nL 368.505674 118.346262 \nL 369.115621 118.145802 \nL 369.725568 118.094406 \nL 369.725568 118.094406 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 65.361932 92.446367 \nL 65.971879 166.573638 \nL 66.581826 141.864544 \nL 67.191773 110.978189 \nL 67.80172 107.271818 \nL 68.411668 104.800912 \nL 69.021615 103.035977 \nL 69.631562 92.446367 \nL 70.241509 84.210005 \nL 71.461403 85.707531 \nL 72.681298 86.74427 \nL 73.291245 97.741172 \nL 73.901192 102.330005 \nL 74.511139 97.079328 \nL 75.731034 80.091823 \nL 76.950928 73.914557 \nL 77.560875 71.267149 \nL 78.780769 73.108819 \nL 79.390717 80.091823 \nL 81.220558 81.464554 \nL 81.830505 79.209361 \nL 83.0504 85.033647 \nL 83.660347 85.272767 \nL 84.270294 87.813417 \nL 84.880241 85.707531 \nL 85.490188 90.266154 \nL 86.710083 90.387282 \nL 87.32003 88.439495 \nL 87.929977 88.544938 \nL 88.539924 86.74427 \nL 90.369766 97.741172 \nL 91.58966 97.500506 \nL 92.199607 99.035455 \nL 92.809554 97.28076 \nL 94.639396 101.523174 \nL 95.249343 104.306732 \nL 95.85929 102.620698 \nL 96.469237 103.850562 \nL 97.079185 103.635389 \nL 97.689132 106.173642 \nL 98.299079 107.271818 \nL 98.909026 105.683385 \nL 99.518973 106.751637 \nL 100.12892 109.061098 \nL 101.348815 110.978189 \nL 101.958762 113.104786 \nL 102.568709 113.967189 \nL 103.178656 111.272339 \nL 104.398551 112.973927 \nL 105.008498 110.416619 \nL 105.618445 110.148405 \nL 106.228392 108.797975 \nL 107.448286 108.330781 \nL 109.278128 113.770651 \nL 109.888075 114.484211 \nL 110.498022 114.19037 \nL 111.717917 115.550973 \nL 112.937811 114.966041 \nL 113.547758 115.611139 \nL 114.157705 117.155456 \nL 114.767652 116.854126 \nL 115.3776 115.666965 \nL 116.597494 115.120594 \nL 117.817388 118.007497 \nL 118.427335 116.874671 \nL 119.037283 117.433093 \nL 119.64723 115.508192 \nL 120.257177 115.254757 \nL 120.867124 115.812571 \nL 122.087018 113.738242 \nL 123.306913 114.838981 \nL 123.91686 114.608134 \nL 124.526807 115.894796 \nL 125.136754 114.909177 \nL 125.746701 114.684549 \nL 126.356649 112.996504 \nL 126.966596 112.795028 \nL 127.576543 114.036833 \nL 128.796437 115.037541 \nL 129.406384 114.1251 \nL 131.236226 115.568635 \nL 131.846173 115.358433 \nL 133.066067 116.272994 \nL 133.676015 116.06214 \nL 134.285962 116.505221 \nL 134.895909 115.651423 \nL 135.505856 114.173326 \nL 136.72575 115.061466 \nL 137.335698 113.625586 \nL 137.945645 113.449096 \nL 138.555592 112.662898 \nL 139.165539 112.497189 \nL 139.775486 113.539495 \nL 140.385433 112.771589 \nL 140.995381 110.829932 \nL 141.605328 110.684028 \nL 142.215275 109.956749 \nL 142.825222 110.978189 \nL 143.435169 110.259901 \nL 144.045116 110.122867 \nL 145.265011 112.101329 \nL 146.484905 111.807974 \nL 147.094852 112.213643 \nL 148.924694 111.783916 \nL 149.534641 112.711378 \nL 150.754535 112.423933 \nL 151.364482 112.805268 \nL 151.97443 113.699637 \nL 152.584377 113.037274 \nL 153.194324 113.917715 \nL 155.634113 115.331293 \nL 156.854007 115.028185 \nL 157.463954 115.854976 \nL 158.683848 115.550973 \nL 159.293796 115.880149 \nL 160.51369 115.581636 \nL 161.123637 114.496892 \nL 162.953479 115.467256 \nL 163.563426 114.867578 \nL 164.78332 114.59415 \nL 165.393267 115.358433 \nL 166.003214 115.220415 \nL 166.613162 115.52792 \nL 167.223109 116.272994 \nL 168.443003 115.992684 \nL 169.662897 116.58083 \nL 170.272845 116.441321 \nL 170.882792 116.72944 \nL 171.492739 116.590683 \nL 172.102686 116.874671 \nL 173.32258 118.265981 \nL 173.932528 118.535852 \nL 174.542475 117.979099 \nL 175.152422 118.657113 \nL 175.762369 118.105805 \nL 176.372316 117.96559 \nL 177.592211 118.491084 \nL 178.202158 118.351056 \nL 178.812105 119.005333 \nL 179.422052 118.864068 \nL 180.031999 118.332079 \nL 181.251894 118.061026 \nL 181.861841 118.313699 \nL 182.471788 118.179669 \nL 183.081735 118.429128 \nL 183.691682 117.91574 \nL 184.301629 118.542193 \nL 184.911577 118.409731 \nL 186.741418 116.908372 \nL 187.351365 116.049084 \nL 187.961312 116.299206 \nL 188.57126 116.1817 \nL 189.791154 115.226943 \nL 190.401101 115.476203 \nL 191.011048 115.36495 \nL 191.620995 115.611139 \nL 192.84089 115.390521 \nL 193.450837 115.633098 \nL 194.670731 115.415385 \nL 195.890626 116.58083 \nL 196.500573 116.812273 \nL 197.11052 116.358389 \nL 198.940361 118.053967 \nL 200.160256 118.491084 \nL 200.770203 118.374296 \nL 201.38015 118.589469 \nL 201.990097 118.473278 \nL 203.209992 117.59086 \nL 203.819939 117.480579 \nL 204.429886 117.04756 \nL 205.039833 117.262887 \nL 205.64978 116.834564 \nL 208.699516 117.888357 \nL 209.309463 117.781003 \nL 209.91941 117.986013 \nL 210.529358 116.948689 \nL 211.139305 117.155456 \nL 211.749252 117.668092 \nL 212.359199 117.257563 \nL 212.969146 117.460508 \nL 213.579093 117.357992 \nL 214.798988 117.758116 \nL 216.628829 116.560064 \nL 217.238776 117.056618 \nL 217.848724 116.367922 \nL 219.068618 116.178817 \nL 219.678565 116.669064 \nL 220.288512 116.864763 \nL 220.898459 116.769382 \nL 221.508407 116.96317 \nL 222.118354 116.58083 \nL 222.728301 116.773856 \nL 223.948195 116.587435 \nL 225.16809 116.967555 \nL 225.778037 117.436241 \nL 226.387984 117.341942 \nL 226.997931 117.805701 \nL 227.607878 117.988355 \nL 228.217825 117.616453 \nL 229.43772 118.528187 \nL 230.047667 118.431944 \nL 230.657614 118.063886 \nL 231.267561 117.155456 \nL 231.877508 117.065278 \nL 232.487456 116.7062 \nL 233.097403 116.886888 \nL 233.70735 116.798654 \nL 234.317297 116.444403 \nL 234.927244 116.624085 \nL 236.147139 117.507188 \nL 236.757086 117.681182 \nL 237.367033 118.115879 \nL 237.97698 117.503477 \nL 239.196874 117.846626 \nL 240.416769 118.699772 \nL 241.026716 118.608932 \nL 241.636663 118.263109 \nL 242.856557 118.593998 \nL 244.686399 118.328091 \nL 245.296346 118.491084 \nL 245.906293 118.403391 \nL 246.51624 118.067543 \nL 247.126188 117.981849 \nL 247.736135 118.143825 \nL 248.956029 116.991823 \nL 250.175923 117.318017 \nL 250.785871 117.236477 \nL 251.395818 117.397702 \nL 252.005765 116.833514 \nL 252.615712 116.51366 \nL 253.225659 116.435776 \nL 253.835606 116.597509 \nL 254.445554 116.996562 \nL 256.275395 116.762004 \nL 257.495289 117.546423 \nL 258.105237 117.467246 \nL 258.715184 117.854778 \nL 259.325131 118.007497 \nL 259.935078 117.927614 \nL 260.545025 118.079163 \nL 261.154972 118.459979 \nL 261.76492 118.379444 \nL 262.984814 117.763683 \nL 263.594761 117.913409 \nL 264.204708 118.288901 \nL 264.814655 118.210111 \nL 265.424603 118.582433 \nL 266.644497 118.872408 \nL 267.254444 118.792811 \nL 267.864391 118.936297 \nL 268.474338 118.635044 \nL 269.084286 118.556873 \nL 269.694233 118.037929 \nL 270.30418 117.961989 \nL 270.914127 118.105805 \nL 272.134021 118.826954 \nL 272.743969 118.532207 \nL 273.353916 118.455936 \nL 275.793704 119.012215 \nL 277.013599 118.859539 \nL 277.623546 119.208654 \nL 278.233493 118.920391 \nL 278.84344 119.056155 \nL 279.453387 119.401746 \nL 280.673282 119.668248 \nL 281.283229 119.591568 \nL 282.503123 120.270047 \nL 283.11307 120.399393 \nL 283.723018 120.734491 \nL 284.332965 120.861827 \nL 284.942912 120.783114 \nL 285.552859 120.500065 \nL 286.162806 120.831197 \nL 286.772753 120.549561 \nL 287.382701 120.67566 \nL 289.212542 120.445532 \nL 289.822489 120.168768 \nL 291.042384 120.418922 \nL 292.262278 121.063866 \nL 292.872225 120.987352 \nL 294.092119 121.229829 \nL 294.702067 120.956854 \nL 297.141855 121.435779 \nL 297.751802 121.747989 \nL 298.36175 121.86502 \nL 298.971697 121.595369 \nL 299.581644 121.519661 \nL 300.191591 121.060265 \nL 300.801538 120.794779 \nL 303.851274 121.378694 \nL 304.461221 121.305074 \nL 305.071168 120.855553 \nL 305.681116 120.783633 \nL 306.291063 121.086455 \nL 306.90101 121.201033 \nL 307.510957 121.128782 \nL 308.120904 120.685326 \nL 308.730851 120.985363 \nL 309.340799 120.729343 \nL 309.950746 120.84338 \nL 311.17064 120.7028 \nL 311.780587 120.450005 \nL 312.390534 120.746188 \nL 313.000482 120.494531 \nL 313.610429 120.607464 \nL 314.220376 120.357375 \nL 315.44027 120.221556 \nL 316.050217 120.334057 \nL 318.490006 121.491428 \nL 319.7099 121.35246 \nL 321.539742 121.6747 \nL 322.149689 121.956755 \nL 322.759636 122.062232 \nL 323.369583 121.817556 \nL 324.589478 122.027669 \nL 325.199425 121.611198 \nL 326.419319 122.166394 \nL 327.029266 121.752496 \nL 327.639214 122.028487 \nL 328.249161 121.960003 \nL 328.859108 121.549451 \nL 329.469055 121.653194 \nL 330.079002 121.586057 \nL 331.908844 120.878749 \nL 333.128738 120.749513 \nL 333.738685 120.349146 \nL 334.348632 120.621437 \nL 334.95858 120.557835 \nL 335.568527 120.661478 \nL 336.178474 120.931228 \nL 338.008315 121.236877 \nL 338.618263 121.172756 \nL 339.22821 121.273637 \nL 339.838157 121.209726 \nL 340.448104 121.474086 \nL 341.667998 121.672756 \nL 342.277946 121.608524 \nL 342.887893 121.869692 \nL 343.49784 121.643109 \nL 344.717734 121.838885 \nL 345.327681 121.774985 \nL 345.937629 121.550567 \nL 346.547576 121.808465 \nL 347.157523 121.745051 \nL 348.377417 121.937867 \nL 349.597312 122.446483 \nL 350.207259 122.223987 \nL 350.817206 122.476605 \nL 351.427153 122.412716 \nL 352.0371 122.034319 \nL 352.647048 122.285733 \nL 353.256995 122.379368 \nL 355.086836 122.190717 \nL 355.696783 122.439159 \nL 356.916678 122.313933 \nL 357.526625 122.560575 \nL 358.136572 122.189745 \nL 358.746519 122.281823 \nL 359.356466 121.913113 \nL 359.966414 122.15854 \nL 361.186308 122.341315 \nL 361.796255 122.584357 \nL 362.406202 122.5226 \nL 363.016149 122.612678 \nL 363.626097 122.55112 \nL 364.236044 122.640779 \nL 364.845991 122.428743 \nL 365.455938 122.518281 \nL 366.065885 122.30735 \nL 366.675832 122.546536 \nL 367.28578 122.48585 \nL 367.895727 122.276256 \nL 368.505674 122.216354 \nL 369.115621 122.008151 \nL 369.725568 122.09728 \nL 369.725568 122.09728 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#p5b751ee2cb)\" d=\"M 50.14375 116.908369 \nL 384.94375 116.908369 \n\" style=\"fill:none;stroke:#000000;stroke-dasharray:5.55,2.4;stroke-dashoffset:0;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 50.14375 251.82 \nL 50.14375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 384.94375 251.82 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 50.14375 251.82 \nL 384.94375 251.82 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 50.14375 7.2 \nL 384.94375 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 302.089063 103.26875 \nL 377.94375 103.26875 \nQ 379.94375 103.26875 379.94375 101.26875 \nL 379.94375 14.2 \nQ 379.94375 12.2 377.94375 12.2 \nL 302.089063 12.2 \nQ 300.089063 12.2 300.089063 14.2 \nL 300.089063 101.26875 \nQ 300.089063 103.26875 302.089063 103.26875 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"line2d_21\">\n     <path d=\"M 304.089063 20.298437 \nL 324.089063 20.298437 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_22\"/>\n    <g id=\"text_16\">\n     <!-- P(die=1) -->\n     <g transform=\"translate(332.089063 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 19.671875 64.796875 \nL 19.671875 37.40625 \nL 32.078125 37.40625 \nQ 38.96875 37.40625 42.71875 40.96875 \nQ 46.484375 44.53125 46.484375 51.125 \nQ 46.484375 57.671875 42.71875 61.234375 \nQ 38.96875 64.796875 32.078125 64.796875 \nz\nM 9.8125 72.90625 \nL 32.078125 72.90625 \nQ 44.34375 72.90625 50.609375 67.359375 \nQ 56.890625 61.8125 56.890625 51.125 \nQ 56.890625 40.328125 50.609375 34.8125 \nQ 44.34375 29.296875 32.078125 29.296875 \nL 19.671875 29.296875 \nL 19.671875 0 \nL 9.8125 0 \nz\n\" id=\"DejaVuSans-80\"/>\n       <path d=\"M 31 75.875 \nQ 24.46875 64.65625 21.28125 53.65625 \nQ 18.109375 42.671875 18.109375 31.390625 \nQ 18.109375 20.125 21.3125 9.0625 \nQ 24.515625 -2 31 -13.1875 \nL 23.1875 -13.1875 \nQ 15.875 -1.703125 12.234375 9.375 \nQ 8.59375 20.453125 8.59375 31.390625 \nQ 8.59375 42.28125 12.203125 53.3125 \nQ 15.828125 64.359375 23.1875 75.875 \nz\n\" id=\"DejaVuSans-40\"/>\n       <path d=\"M 10.59375 45.40625 \nL 73.1875 45.40625 \nL 73.1875 37.203125 \nL 10.59375 37.203125 \nz\nM 10.59375 25.484375 \nL 73.1875 25.484375 \nL 73.1875 17.1875 \nL 10.59375 17.1875 \nz\n\" id=\"DejaVuSans-61\"/>\n       <path d=\"M 8.015625 75.875 \nL 15.828125 75.875 \nQ 23.140625 64.359375 26.78125 53.3125 \nQ 30.421875 42.28125 30.421875 31.390625 \nQ 30.421875 20.453125 26.78125 9.375 \nQ 23.140625 -1.703125 15.828125 -13.1875 \nL 8.015625 -13.1875 \nQ 14.5 -2 17.703125 9.0625 \nQ 20.90625 20.125 20.90625 31.390625 \nQ 20.90625 42.671875 17.703125 53.65625 \nQ 14.5 64.65625 8.015625 75.875 \nz\n\" id=\"DejaVuSans-41\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-49\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_23\">\n     <path d=\"M 304.089063 34.976562 \nL 324.089063 34.976562 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_24\"/>\n    <g id=\"text_17\">\n     <!-- P(die=2) -->\n     <g transform=\"translate(332.089063 38.476562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-50\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 304.089063 49.654687 \nL 324.089063 49.654687 \n\" style=\"fill:none;stroke:#2ca02c;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_26\"/>\n    <g id=\"text_18\">\n     <!-- P(die=3) -->\n     <g transform=\"translate(332.089063 53.154687)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-51\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 304.089063 64.332812 \nL 324.089063 64.332812 \n\" style=\"fill:none;stroke:#d62728;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_28\"/>\n    <g id=\"text_19\">\n     <!-- P(die=4) -->\n     <g transform=\"translate(332.089063 67.832812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_29\">\n     <path d=\"M 304.089063 79.010937 \nL 324.089063 79.010937 \n\" style=\"fill:none;stroke:#9467bd;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_30\"/>\n    <g id=\"text_20\">\n     <!-- P(die=5) -->\n     <g transform=\"translate(332.089063 82.510937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-53\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n    <g id=\"line2d_31\">\n     <path d=\"M 304.089063 93.689062 \nL 324.089063 93.689062 \n\" style=\"fill:none;stroke:#8c564b;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_32\"/>\n    <g id=\"text_21\">\n     <!-- P(die=6) -->\n     <g transform=\"translate(332.089063 97.189062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 33.015625 40.375 \nQ 26.375 40.375 22.484375 35.828125 \nQ 18.609375 31.296875 18.609375 23.390625 \nQ 18.609375 15.53125 22.484375 10.953125 \nQ 26.375 6.390625 33.015625 6.390625 \nQ 39.65625 6.390625 43.53125 10.953125 \nQ 47.40625 15.53125 47.40625 23.390625 \nQ 47.40625 31.296875 43.53125 35.828125 \nQ 39.65625 40.375 33.015625 40.375 \nz\nM 52.59375 71.296875 \nL 52.59375 62.3125 \nQ 48.875 64.0625 45.09375 64.984375 \nQ 41.3125 65.921875 37.59375 65.921875 \nQ 27.828125 65.921875 22.671875 59.328125 \nQ 17.53125 52.734375 16.796875 39.40625 \nQ 19.671875 43.65625 24.015625 45.921875 \nQ 28.375 48.1875 33.59375 48.1875 \nQ 44.578125 48.1875 50.953125 41.515625 \nQ 57.328125 34.859375 57.328125 23.390625 \nQ 57.328125 12.15625 50.6875 5.359375 \nQ 44.046875 -1.421875 33.015625 -1.421875 \nQ 20.359375 -1.421875 13.671875 8.265625 \nQ 6.984375 17.96875 6.984375 36.375 \nQ 6.984375 53.65625 15.1875 63.9375 \nQ 23.390625 74.21875 37.203125 74.21875 \nQ 40.921875 74.21875 44.703125 73.484375 \nQ 48.484375 72.75 52.59375 71.296875 \nz\n\" id=\"DejaVuSans-54\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-80\"/>\n      <use x=\"60.302734\" xlink:href=\"#DejaVuSans-40\"/>\n      <use x=\"99.316406\" xlink:href=\"#DejaVuSans-100\"/>\n      <use x=\"162.792969\" xlink:href=\"#DejaVuSans-105\"/>\n      <use x=\"190.576172\" xlink:href=\"#DejaVuSans-101\"/>\n      <use x=\"252.099609\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"335.888672\" xlink:href=\"#DejaVuSans-54\"/>\n      <use x=\"399.511719\" xlink:href=\"#DejaVuSans-41\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p5b751ee2cb\">\n   <rect height=\"244.62\" width=\"334.8\" x=\"50.14375\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Axioms of Probability Theory**"
      ],
      "metadata": {
        "id": "DJ4TB6095eqf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When dealing with the rolls of a die, we call the set S={1,2,3,4,5,6} the sample space or outcome space, where each element is an outcome. An event is a set of outcomes from a given sample space. For instance, ‚Äúseeing a 5‚Äù ({5}) and ‚Äúseeing an odd number‚Äù ({1,3,5}) are both valid events of rolling a die. Note that if the outcome of a random experiment is in event A, then event A has occurred. That is to say, if 3 dots faced up after rolling a die, since 3‚àà{1,3,5}, we can say that the event ‚Äúseeing an odd number‚Äù has occurred.\n",
        "\n",
        "Formally, probability can be thought of as a function that maps a set to a real value. The probability of an event A in the given sample space S, denoted as P(A), satisfies the following properties:\n",
        "\n",
        "* For any event A, its probability is never negative, i.e., P(A)‚â•0;\n",
        "\n",
        "* Probability of the entire sample space is 1, i.e., P(S)=1;\n",
        "\n",
        "* For any countable sequence of events A1,A2,‚Ä¶\n",
        "that are mutually exclusive (Ai‚à©Aj=‚àÖ for all i‚â†j), the probability that any happens is equal to the sum of their individual probabilities, i.e., P(‚ãÉ‚àûi=1Ai)=‚àë‚àûi=1P(Ai)."
      ],
      "metadata": {
        "id": "-NvEohvm5i6C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Random Variables**"
      ],
      "metadata": {
        "id": "vRlpqsKr5xlj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our random experiment of casting a die, we introduced the notion of a random variable. A random variable can be pretty much any quantity and is not deterministic. It could take one value among a set of possibilities in a random experiment. Consider a random variable X whose value is in the sample space S={1,2,3,4,5,6} of rolling a die. We can denote the event ‚Äúseeing a 5‚Äù as {X=5} or X=5, and its probability as P({X=5}) or P(X=5). By P(X=a), we make a distinction between the random variable X and the values (e.g., a) that X can take. However, such pedantry results in a cumbersome notation. For a compact notation, on one hand, we can just denote P(X) as the distribution over the random variable X: the distribution tells us the probability that X takes any value. On the other hand, we can simply write P(a) to denote the probability that a random variable takes the value a. Since an event in probability theory is a set of outcomes from the sample space, we can specify a range of values for a random variable to take. For example, P(1‚â§X‚â§3) denotes the probability of the event {1‚â§X‚â§3}, which means {X=1,2,or,3}. Equivalently, P(1‚â§X‚â§3) represents the probability that the random variable X can take a value from {1,2,3}."
      ],
      "metadata": {
        "id": "Nw6DcEva50PN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Dealing with Multiple Random Variables**"
      ],
      "metadata": {
        "id": "DzlgtvnD51zi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Very often, we will want to consider more than one random variable at a time. For instance, we may want to model the relationship between diseases and symptoms. Given a disease and a symptom, say ‚Äúflu‚Äù and ‚Äúcough‚Äù, either may or may not occur in a patient with some probability. While we hope that the probability of both would be close to zero, we may want to estimate these probabilities and their relationships to each other so that we may apply our inferences to effect better medical care.\n",
        "\n",
        "As a more complicated example, images contain millions of pixels, thus millions of random variables. And in many cases images will come with a label, identifying objects in the image. We can also think of the label as a random variable. We can even think of all the metadata as random variables such as location, time, aperture, focal length, ISO, focus distance, and camera type. All of these are random variables that occur jointly. When we deal with multiple random variables, there are several quantities of interest."
      ],
      "metadata": {
        "id": "tGtKC9k953nD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Expectation and Variance**"
      ],
      "metadata": {
        "id": "xCD_H_tR55zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To summarize key characteristics of probability distributions, we need some measures. The expectation (or average) of the random variable X\n",
        "\n",
        "is denoted as\n",
        "E[X]=‚àëxxP(X=x).\n",
        "\n",
        "When the input of a function f(x)\n",
        "is a random variable drawn from the distribution P with different values x, the expectation of f(x)\n",
        "\n",
        "is computed as\n",
        "Ex‚àºP[f(x)]=‚àëxf(x)P(x).\n",
        "\n",
        "In many cases we want to measure by how much the random variable X\n",
        "\n",
        "deviates from its expectation. This can be quantified by the variance\n",
        "Var[X]=E[(X‚àíE[X])2]=E[X2]‚àíE[X]2.\n",
        "\n",
        "Its square root is called the standard deviation. The variance of a function of a random variable measures by how much the function deviates from the expectation of the function, as different values x\n",
        "\n",
        "of the random variable are sampled from its distribution:\n",
        "Var[f(x)]=E[(f(x)‚àíE[f(x)])2]."
      ],
      "metadata": {
        "id": "fQAOxS8-59l-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**"
      ],
      "metadata": {
        "id": "aaXYWjQt6U3y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We can sample from probability distributions.\n",
        "\n",
        "* We can analyze multiple random variables using joint distribution, conditional distribution, Bayes‚Äô theorem, marginalization, and independence assumptions.\n",
        "\n",
        "* Expectation and variance offer useful measures to summarize key characteristics of probability distributions"
      ],
      "metadata": {
        "id": "ecQ0T-Uy6Was"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Documentation**"
      ],
      "metadata": {
        "id": "O2zzk1va6fhy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Due to constraints on the length of this book, we cannot possibly introduce every single PyTorch function and class (and you probably would not want us to). The API documentation and additional tutorials and examples provide plenty of documentation beyond the book. In this section we provide you with some guidance to exploring the PyTorch API."
      ],
      "metadata": {
        "id": "7fFgNupE6mHE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Finding All the Functions and Classes in a Module**"
      ],
      "metadata": {
        "id": "ZFq85v4k6nCx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "n order to know which functions and classes can be called in a module, we invoke the dir function. For instance, we can query all properties in the module for generating random numbers:"
      ],
      "metadata": {
        "id": "LGS41Qfh6oR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(dir(torch.distributions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LyaxWwWH6pYP",
        "outputId": "df7f0242-c27f-4b9c-dba6-87201d710feb"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['AbsTransform', 'AffineTransform', 'Bernoulli', 'Beta', 'Binomial', 'CatTransform', 'Categorical', 'Cauchy', 'Chi2', 'ComposeTransform', 'ContinuousBernoulli', 'CorrCholeskyTransform', 'Dirichlet', 'Distribution', 'ExpTransform', 'Exponential', 'ExponentialFamily', 'FisherSnedecor', 'Gamma', 'Geometric', 'Gumbel', 'HalfCauchy', 'HalfNormal', 'Independent', 'IndependentTransform', 'Kumaraswamy', 'LKJCholesky', 'Laplace', 'LogNormal', 'LogisticNormal', 'LowRankMultivariateNormal', 'LowerCholeskyTransform', 'MixtureSameFamily', 'Multinomial', 'MultivariateNormal', 'NegativeBinomial', 'Normal', 'OneHotCategorical', 'OneHotCategoricalStraightThrough', 'Pareto', 'Poisson', 'PowerTransform', 'RelaxedBernoulli', 'RelaxedOneHotCategorical', 'ReshapeTransform', 'SigmoidTransform', 'SoftmaxTransform', 'StackTransform', 'StickBreakingTransform', 'StudentT', 'TanhTransform', 'Transform', 'TransformedDistribution', 'Uniform', 'VonMises', 'Weibull', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'bernoulli', 'beta', 'biject_to', 'binomial', 'categorical', 'cauchy', 'chi2', 'constraint_registry', 'constraints', 'continuous_bernoulli', 'dirichlet', 'distribution', 'exp_family', 'exponential', 'fishersnedecor', 'gamma', 'geometric', 'gumbel', 'half_cauchy', 'half_normal', 'identity_transform', 'independent', 'kl', 'kl_divergence', 'kumaraswamy', 'laplace', 'lkj_cholesky', 'log_normal', 'logistic_normal', 'lowrank_multivariate_normal', 'mixture_same_family', 'multinomial', 'multivariate_normal', 'negative_binomial', 'normal', 'one_hot_categorical', 'pareto', 'poisson', 'register_kl', 'relaxed_bernoulli', 'relaxed_categorical', 'studentT', 'transform_to', 'transformed_distribution', 'transforms', 'uniform', 'utils', 'von_mises', 'weibull']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generally, we can ignore functions that start and end with __ (special objects in Python) or functions that start with a single _(usually internal functions). Based on the remaining function or attribute names, we might hazard a guess that this module offers various methods for generating random numbers, including sampling from the uniform distribution (uniform), normal distribution (normal), and multinomial distribution (multinomial)."
      ],
      "metadata": {
        "id": "Z_IcFhT56st0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Finding the Usage of Specific Functions and Classes**"
      ],
      "metadata": {
        "id": "IqJR1P0F6tRt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more specific instructions on how to use a given function or class, we can invoke the help function. As an example, let us explore the usage instructions for tensors‚Äô ones function."
      ],
      "metadata": {
        "id": "AWQlj8xj6vnM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(torch.ones)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RPaFBIs6wps",
        "outputId": "6f594c37-b759-46df-a9fb-d4edd05c174a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on built-in function ones:\n",
            "\n",
            "ones(...)\n",
            "    ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) -> Tensor\n",
            "    \n",
            "    Returns a tensor filled with the scalar value `1`, with the shape defined\n",
            "    by the variable argument :attr:`size`.\n",
            "    \n",
            "    Args:\n",
            "        size (int...): a sequence of integers defining the shape of the output tensor.\n",
            "            Can be a variable number of arguments or a collection like a list or tuple.\n",
            "    \n",
            "    Keyword arguments:\n",
            "        out (Tensor, optional): the output tensor.\n",
            "        dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.\n",
            "            Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).\n",
            "        layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.\n",
            "            Default: ``torch.strided``.\n",
            "        device (:class:`torch.device`, optional): the desired device of returned tensor.\n",
            "            Default: if ``None``, uses the current device for the default tensor type\n",
            "            (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU\n",
            "            for CPU tensor types and the current CUDA device for CUDA tensor types.\n",
            "        requires_grad (bool, optional): If autograd should record operations on the\n",
            "            returned tensor. Default: ``False``.\n",
            "    \n",
            "    Example::\n",
            "    \n",
            "        >>> torch.ones(2, 3)\n",
            "        tensor([[ 1.,  1.,  1.],\n",
            "                [ 1.,  1.,  1.]])\n",
            "    \n",
            "        >>> torch.ones(5)\n",
            "        tensor([ 1.,  1.,  1.,  1.,  1.])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the documentation, we can see that the ones function creates a new tensor with the specified shape and sets all the elements to the value of 1. Whenever possible, you should run a quick test to confirm your interpretation:"
      ],
      "metadata": {
        "id": "xjpkc8Fy60Dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.ones(4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLXNK7DU62H8",
        "outputId": "45a7089a-9805-4276-9a52-29ff02e387b3"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1., 1., 1., 1.])"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n the Jupyter notebook, we can use ? to display the document in another window. For example, list? will create content that is almost identical to help(list), displaying it in a new browser window. In addition, if we use two question marks, such as list??, the Python code implementing the function will also be displayed."
      ],
      "metadata": {
        "id": "umzRTOMp677j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Summary**"
      ],
      "metadata": {
        "id": "hj2LpflC68gU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The official documentation provides plenty of descriptions and examples that are beyond this book.\n",
        "\n",
        "* We can look up documentation for the usage of an API by calling the dir and help functions, or ? and ?? in Jupyter notebooks"
      ],
      "metadata": {
        "id": "WHIVpaBO6-KQ"
      }
    }
  ]
}